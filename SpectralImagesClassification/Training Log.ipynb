{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model \n",
    "\n",
    "Simple convolutional model no dropout\n",
    "\n",
    "```python\n",
    "def createModel(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "```    \n",
    "\n",
    "## Dropout model\n",
    "\n",
    "Added dropout layers after the conv2d and the dense\n",
    "\n",
    "```python\n",
    "def createModel(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',input_shape=input_shape))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "```\n",
    "\n",
    "## Base LSTM model \n",
    "\n",
    "```python\n",
    "\n",
    "def createLstmModel(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_shape)))\n",
    "    model.add(TimeDistributed(Conv2D(16, (3, 3))))\n",
    "    model.add(TimeDistributed(Activation('relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(TimeDistributed(Dense(45)))\n",
    "    model.add(TimeDistributed(Activation('relu')))\n",
    "    #model.add(Dropout(0.5))\n",
    "    \n",
    "    #LSTM layer\n",
    "    model.add(Bidirectional(LSTM(20, stateful=False, dropout=0.35)))\n",
    "    \n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic Dataset \n",
    "\n",
    "### Multi frame classification\n",
    "\n",
    "#### Parameters\n",
    "```python \n",
    "image_size = 32\n",
    "frame_length = 1\n",
    "sequence_length = 5\n",
    "overlap = 0.5\n",
    "num_classes = 2\n",
    "```\n",
    "Total number of samples \n",
    "(422,)\n",
    "(422, 5, 32, 32, 3)\n",
    "#### No normalization\n",
    "\n",
    "![alt text](trainingGraphs/multiFrame_Arithmetic_noNormalization.png \"Training graph\")\n",
    "\n",
    "#### Global Normalization\n",
    "\n",
    "![alt text](trainingGraphs/multiFrame_Arithmetic_GlobalNormalization.png \"Training graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic Dataset\n",
    "\n",
    "Using Jing and Juan arithmetic data. Total number of samples (1824, 32, 32, 3) (1824,)\n",
    "\n",
    "### Single frame classification\n",
    "\n",
    "\n",
    "#### No normalization\n",
    "![alt text](trainingGraphs/singleFrame_Arithmetic_noNormalization.png \"train graph\")\n",
    "\n",
    "#### Global normalization\n",
    "\n",
    "In the same 400 epochs the model achieved a better performance on the training set. Generalization was equally crappy in both settings\n",
    "\n",
    "![alt text](trainingGraphs/singleFrame_Arithmetic_GlobalNormalization.png \"train graph\")\n",
    "\n",
    "#### Color wise normalization \n",
    "\n",
    "![alt text](trainingGraphs/singleFrame_Arithmetic_ColorNormalization.png \"train graph\")\n",
    "\n",
    "#### 64 pixels images with dropout (Global Normalization)\n",
    "\n",
    "![alt text](trainingGraphs/singleFrame_Arithmetic_GlobalNormalization_Dropout_64px.png \"train graph\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion Dataset\n",
    "\n",
    "Using Juan inversion dataset. Total number of samples (15748, 32, 32, 3) (15748,)\n",
    "### Single frame classification\n",
    "\n",
    "#### Global normalization\n",
    "\n",
    "![alt text](trainingGraphs/singleFrame_inversion_GlobalNormalization.png \"train graph\")\n",
    "\n",
    "#### Dropout model \n",
    "\n",
    "![alt text](trainingGraphs/singleFrame_inversion_GlobalNormalization_Dropout.png \"train graph\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiframe classification\n",
    "\n",
    "Dataset size (451,) (451, 6, 90)\n",
    "\n",
    "### Simple LSTM\n",
    "\n",
    "image_size = 32\n",
    "frame_length = 1\n",
    "sequence_length = 12\n",
    "overlap = 0.5\n",
    "num_classes = 2\n",
    "* Uses new functions to overlape the LSTM training sequences\n",
    "* Train (1454, 12, 90) (1454,)\n",
    "* Test (394, 12, 90) (394,)\n",
    "\n",
    "![alt text](trainingGraphs/simpleLstm_arithmetic.png \"train graph\")\n",
    "\n",
    "### Convolutional LSTM\n",
    "image_size = 32\n",
    "frame_length = 1\n",
    "sequence_length = 6\n",
    "overlap = 0.5\n",
    "num_classes = 2\n",
    "\n",
    "![alt text](trainingGraphs/convolutionalLstm_arithmetic.png \"train graph\")\n",
    "\n",
    "### Simple LSTM with encoded features\n",
    "\n",
    "![alt text](trainingGraphs/simpleLstm_arithmetic_encodedFeatures.png \"train graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eyes open/close dataset classification\n",
    "\n",
    "\n",
    "## Convolutional model\n",
    "\n",
    "64px images\n",
    "\n",
    "![alt text](trainingGraphs/simpleLstm_arithmetic_encodedFeatures323.png \"train graph\")\n",
    "\n",
    "## Convolutional lstm\n",
    "\n",
    "## lstm format\n",
    "\n",
    "seq5\n",
    "\n",
    "![alt text](trainingGraphs/simpleLSTM_eyes_seq5.png \"train graph\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
