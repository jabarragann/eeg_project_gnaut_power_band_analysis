{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with Jing math data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainModel import createImageDataset, showImages, buildTrainAndVal\n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Possible set of labels\n",
    "# \"\"\"\n",
    "# [\"pegNormal\",\"pegInversion\"]\n",
    "# [\"EasyAdd\",\"HardMult\"]\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataPath = Path(r\"C:\\Users\\asus\\OneDrive - purdue.edu\\RealtimeProject\\Experiment1-Pilot\\UI02\\pyprep_edf\")\n",
    "# X, y = createImageDataset(dataPath,imageSize=32,frameDuration=1,overlap=0.2,\n",
    "#                           augment_data=False, labels = [\"pegNormal\",\"pegInversion\"],\n",
    "#                           lstm_format=False, lstm_sequence_length=None,\n",
    "#                          fileNameFormat=1)\n",
    "\n",
    "\n",
    "# Eyes open-close dataset\n",
    "lstm_format = True\n",
    "lstm_length = 4\n",
    "\n",
    "dataPath = Path(r'C:\\Users\\asus\\OneDrive - purdue.edu\\RealtimeProject\\Experiments3-Data\\EyesOpen_closeDataset\\edf')\n",
    "X, y = createImageDataset(dataPath,imageSize=64,frameDuration=1,overlap=0.5,\n",
    "                          augment_data=False, labels = [\"EyesClosed\",\"EyesOpen\"],\n",
    "                          lstm_format=False, lstm_sequence_length=lstm_length,\n",
    "                          fileNameFormat=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataPath = Path(r\"C:\\Users\\asus\\OneDrive - purdue.edu\\RealtimeProject\\Experiments3-Data\\ArithmeticCalibrationProcedure\\edf\")\n",
    "X_arith, y_arith = createImageDataset(dataPath,imageSize=64,frameDuration=1,overlap=0.2,\n",
    "                          image_format=False, augment_data=False, labels = [\"EasyAdd\",\"HardMult\"],\n",
    "                          lstm_format=False, lstm_sequence_length=None,\n",
    "                         fileNameFormat=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Arithmetic dataset size\", )\n",
    "# print(X_arith.shape, y_arith.shape)\n",
    "\n",
    "print(\"Inversion/eyes-data dataset size\", )\n",
    "print(X.shape, y.shape)\n",
    "showImages(X,y)\n",
    "\n",
    "# X.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, TimeDistributed, Input, BatchNormalization, Softmax\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def createConvModel(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',input_shape=input_shape))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def createLstmModel(input_shape,num_classes, lstmLayers=2, lstmOutputSize=4.0,\n",
    "                  isBidirectional=1.0, inputLayerNeurons=64, inputLayerDropout=0.25):\n",
    "\n",
    "    dropoutRate = 0.45\n",
    "    \n",
    "    lstmLayers = int(lstmLayers)\n",
    "    lstmOutputSize =int(lstmOutputSize)\n",
    "    isBidirectional =int(isBidirectional)\n",
    "\n",
    "    # Input layer\n",
    "    networkInput = Input(shape=input_shape)\n",
    "    dropout1 = Dropout(rate=inputLayerDropout)(networkInput)\n",
    "\n",
    "    # First Hidden layer\n",
    "    hidden1 = Dense(inputLayerNeurons, activation='relu')(dropout1)\n",
    "    dropout2 = Dropout(rate=dropoutRate)(hidden1)\n",
    "    batchNorm1 = BatchNormalization()(dropout2)\n",
    "\n",
    "    out = batchNorm1\n",
    "    for i in range(1, lstmLayers+1):\n",
    "        retSeq = False if i == lstmLayers else True\n",
    "        lstmLayer = LSTM(lstmOutputSize, stateful=False, return_sequences=retSeq,\n",
    "                         dropout=dropoutRate, kernel_regularizer=regularizers.l2(0.05))\n",
    "        if isBidirectional:\n",
    "            out = Bidirectional(lstmLayer, merge_mode='concat')(out)\n",
    "        else:\n",
    "            out = lstmLayer(out)\n",
    "\n",
    "    hidden3 = Dense(num_classes, activation='linear')(out)\n",
    "    networkOutput = Softmax()(hidden3)\n",
    "\n",
    "    model1 = Model(inputs=networkInput, outputs=networkOutput)\n",
    "    model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model1\n",
    "\n",
    "def createConvLstmModel(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_shape)))\n",
    "#     model.add(TimeDistributed(Conv2D(8, (3, 3), padding='same')))\n",
    "# #     model.add(Dropout(rate=0.3))\n",
    "#     model.add(TimeDistributed(Activation('relu')))\n",
    "#     model.add(Dropout(rate=0.3))\n",
    "    model.add(TimeDistributed(Conv2D(16, (3, 3))))\n",
    "    model.add(TimeDistributed(Activation('relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(TimeDistributed(Dense(45)))\n",
    "    model.add(TimeDistributed(Activation('relu')))\n",
    "    #model.add(Dropout(0.5))\n",
    "    \n",
    "    #LSTM layer\n",
    "    model.add(Bidirectional(LSTM(20, stateful=False, dropout=0.35)))\n",
    "    \n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    " \n",
    "def createConvLstmModel(input_shape, num_classes):\n",
    "    networkInput = Input(shape=input_shape)\n",
    "    \n",
    "#     model.add(TimeDistributed(Conv2D(8, (3, 3), padding='same')))\n",
    "# #     model.add(Dropout(rate=0.3))\n",
    "#     model.add(TimeDistributed(Activation('relu')))\n",
    "#     model.add(Dropout(rate=0.3))\n",
    "    x = TimeDistributed(Conv2D(16, (3, 3))) (network_input) \n",
    "    x = TimeDistributed(Activation('relu')) (x)\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "    x = TimeDistributed(Flatten()) (x)\n",
    "    x = TimeDistributed(Dense(45)) (x)\n",
    "    x = TimeDistributed(Activation('relu')) (x)\n",
    "    #model.add(Dropout(0.5))\n",
    "    \n",
    "    #LSTM layer\n",
    "    x = Bidirectional(LSTM(20, stateful=False, dropout=0.35)) (x)\n",
    "    \n",
    "    x = Dense(num_classes) (x) \n",
    "    networkOutput = Activation('softmax') (x)\n",
    "    \n",
    "    model = Model(inputs=networkInput, outputs=networkOutput)\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model summary\n",
    "testModel = createConvModel((64,64,3),2)\n",
    "testModel.summary()\n",
    "\n",
    "testModel = createLstmModel((3, 90),2)\n",
    "testModel.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train convolutional network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20,shuffle=True)\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "img_rows, img_cols = 64, 64\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "#x_train /= 255\n",
    "#x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "input_shape = (img_rows, img_cols, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get dataset statistics\n",
    "print(x_train.shape)\n",
    "print(\"Global statistics\")\n",
    "print(x_train.mean())\n",
    "print(x_train.std())\n",
    "print(\"Statistics per color\")\n",
    "print(x_train.mean(axis=(0, 1,2)))\n",
    "print(x_train.std(axis=(0,1,2)))\n",
    "\n",
    "##Normalize the dataset\n",
    "\n",
    "##No normalization -- Seems to yield the best results \n",
    "# x_train_2 = x_train\n",
    "# x_test_2 = x_test\n",
    "\n",
    "##Method 1: Normalize with global mean and std\n",
    "global_mean = x_train.mean(); global_std = x_train.std();\n",
    "x_train_2 = (x_train - global_mean)/global_std;\n",
    "x_test_2 = (x_test - global_mean)/global_std;\n",
    "\n",
    "##Method 2: Normalize each color channel independently (e.g normalize the red channel with red mean accross all samples)\n",
    "# color_mean = x_train.mean(axis=(0, 1,2)); color_std = x_train.std(axis=(0, 1,2));\n",
    "# color_mean = np.expand_dims(color_mean,axis=(0,1,2))\n",
    "# x_train_2 = (x_train - color_mean)/color_std;\n",
    "# x_test_2 = (x_test - color_mean)/color_std;\n",
    "\n",
    "\n",
    "##Debug\n",
    "x_train_2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 150\n",
    "img_size = 64\n",
    "input_shape = (img_size, img_size, 3)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train_2 = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_2 = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(x_train.shape)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "model = createConvModel(input_shape, num_classes)\n",
    "\n",
    "history = model.fit(x_train_2, y_train_2,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test_2, y_test_2),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.set_title(\"eyes dataset, global normalization, 64px img\")\n",
    "ax.set_ylim((0.4,1))\n",
    "ax.set_yticks(np.arange(0.4,1,0.05))\n",
    "ax.grid()\n",
    "ax.plot(history.history['accuracy'])\n",
    "ax.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "import pickle\n",
    "## Save model for real-time\n",
    "# model.summary()\n",
    "# print(global_mean, global_std)\n",
    "model.save('./models/'+'conv_eyes-open-close_64px.h5')\n",
    "normalizer_dict = {'mean':global_mean,'std':global_std}\n",
    "image_size=64;frame_length=1;sequence_length=-1;overlap=0.5\n",
    "config_dict = {'image_size':image_size,'frame_length':frame_length,'sequence_length':sequence_length,'overlap':overlap}\n",
    "\n",
    "pickle.dump(normalizer_dict, open('./models/'+'conv_eyes-open-close_64px_normalizer.pickle','wb'))\n",
    "pickle.dump(config_dict,     open('./models/'+'conv_eyes-open-close_64px_config.pickle','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Convolutional LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading arithmetic calibration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "frame_length = 1\n",
    "sequence_length = 6\n",
    "overlap = 0.5\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataPath = Path(r\"C:\\Users\\asus\\OneDrive - purdue.edu\\RealtimeProject\\Experiments3-Data\\ArithmeticCalibrationProcedure\\edf\")\n",
    "X_arith_seq, y_arith_seq = createImageDataset(dataPath,imageSize=image_size,frameDuration=frame_length,overlap=overlap,\n",
    "                           augment_data=False, image_format = False, encoded_format = False, labels = [\"EasyAdd\",\"HardMult\"],\n",
    "                           lstm_format=True, lstm_sequence_length=sequence_length,\n",
    "                           fileNameFormat=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataPath = Path(r\"C:\\Users\\asus\\OneDrive - purdue.edu\\RealtimeProject\\Experiment1-Pilot\\UI02\\pyprep_edf\")\n",
    "# X_inver_seq, y_inver_seq = createImageDataset(dataPath,imageSize=32,frameDuration=1,overlap=0.2,\n",
    "#                           augment_data=False, labels = [\"pegNormal\",\"pegInversion\"],\n",
    "#                           lstm_format=False, lstm_sequence_length=None,\n",
    "#                          fileNameFormat=1)   \n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_arith_seq, y_arith_seq, test_size=0.20,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Normalize the dataset\n",
    "\n",
    "##No normalization -- Seems to yield the best results \n",
    "# x_train_2 = x_train\n",
    "# x_test_2 = x_test\n",
    "\n",
    "##Global frame/time normalization\n",
    "global_mean = x_train.mean(); global_std = x_train.std();\n",
    "x_train_2 = (x_train - global_mean)/global_std;\n",
    "x_test_2 = (x_test - global_mean)/global_std;\n",
    "\n",
    "## convert class vectors to binary class matrices\n",
    "y_train_2 = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_2 = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train_2 = x_train_2.astype('float32')\n",
    "x_test_2 = x_test_2.astype('float32')\n",
    "\n",
    "print(y_train.shape)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##Training\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 600\n",
    "input_shape = (sequence_length, image_size,image_size,3)\n",
    "model = createConvLstmModel(input_shape, num_classes)\n",
    "\n",
    "history = model.fit(x_train_2, y_train_2,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test_2, y_test_2),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.set_title(\"LSTM classification, arithmetic\")\n",
    "ax.set_ylim((0.4,1))\n",
    "ax.set_yticks(np.arange(0.4,1,0.05))\n",
    "ax.grid()\n",
    "ax.plot(history.history['accuracy'])\n",
    "ax.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Simple LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "frame_length = 1\n",
    "sequence_length = 20\n",
    "overlap = 0.5\n",
    "num_classes = 2\n",
    "\n",
    "dataPath = Path(r\"C:\\Users\\asus\\OneDrive - purdue.edu\\RealtimeProject\\Experiments3-Data\\ArithmeticCalibrationProcedure\\edf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_arith_seq, y_arith_seq = createImageDataset(dataPath,imageSize=image_size,frameDuration=frame_length,overlap=overlap,\n",
    "                           image_format=False,encoded_format=False, augment_data=False, labels = [\"EasyAdd\",\"HardMult\"],\n",
    "                           lstm_format=True, lstm_sequence_length=sequence_length,\n",
    "                           fileNameFormat=2)\n",
    "\n",
    "# dataPath = Path(r'C:\\Users\\asus\\OneDrive - purdue.edu\\RealtimeProject\\Experiments3-Data\\EyesOpen_closeDataset\\edf')\n",
    "# X_eyes, y_eyes = createImageDataset(dataPath,imageSize=64,frameDuration=1,overlap=0.5,\n",
    "#                           image_format= False, augment_data=False, labels = [\"EyesClosed\",\"EyesOpen\"],\n",
    "#                           lstm_format=True, lstm_sequence_length=sequence_length,\n",
    "#                          fileNameFormat=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train\n",
      "file information: UJing 2 7 EasyAdd\n",
      "file information: UJing 3 9 EasyAdd\n",
      "file information: UJing 2 1 EasyAdd\n",
      "file information: UJuan 1 11 EasyAdd\n",
      "file information: UJing 2 3 EasyAdd\n",
      "file information: UJing 1 11 EasyAdd\n",
      "file information: UJing 2 11 EasyAdd\n",
      "file information: UJing 2 9 EasyAdd\n",
      "file information: UJing 3 1 EasyAdd\n",
      "file information: UJuan 1 1 EasyAdd\n",
      "file information: UJuan 2 11 EasyAdd\n",
      "file information: UJuan 3 3 EasyAdd\n",
      "file information: UJuan 2 9 EasyAdd\n",
      "file information: UJing 2 5 EasyAdd\n",
      "file information: UJuan 3 7 EasyAdd\n",
      "file information: UJuan 2 7 EasyAdd\n",
      "file information: UJing 3 3 EasyAdd\n",
      "file information: UJuan 1 3 EasyAdd\n",
      "file information: UJing 1 1 EasyAdd\n",
      "file information: UJing 1 7 EasyAdd\n",
      "file information: UJuan 3 1 EasyAdd\n",
      "file information: UJuan 3 11 EasyAdd\n",
      "file information: UJuan 3 5 EasyAdd\n",
      "file information: UJuan 2 5 EasyAdd\n",
      "file information: UJuan 1 5 EasyAdd\n",
      "file information: UJuan 1 9 EasyAdd\n",
      "file information: UJuan 1 7 EasyAdd\n",
      "file information: UJing 3 7 EasyAdd\n",
      "file information: UJing 2 8 HardMult\n",
      "file information: UJing 3 10 HardMult\n",
      "file information: UJing 2 2 HardMult\n",
      "file information: UJuan 1 12 HardMult\n",
      "file information: UJing 2 4 HardMult\n",
      "file information: UJing 1 12 HardMult\n",
      "file information: UJing 2 12 HardMult\n",
      "file information: UJing 2 10 HardMult\n",
      "file information: UJing 3 2 HardMult\n",
      "file information: UJuan 1 2 HardMult\n",
      "file information: UJuan 2 12 HardMult\n",
      "file information: UJuan 3 4 HardMult\n",
      "file information: UJuan 2 10 HardMult\n",
      "file information: UJing 2 6 HardMult\n",
      "file information: UJuan 3 8 HardMult\n",
      "file information: UJuan 2 8 HardMult\n",
      "file information: UJing 3 4 HardMult\n",
      "file information: UJuan 1 4 HardMult\n",
      "file information: UJing 1 2 HardMult\n",
      "file information: UJing 1 8 HardMult\n",
      "file information: UJuan 3 2 HardMult\n",
      "file information: UJuan 3 12 HardMult\n",
      "file information: UJuan 3 6 HardMult\n",
      "file information: UJuan 2 6 HardMult\n",
      "file information: UJuan 1 6 HardMult\n",
      "file information: UJuan 1 10 HardMult\n",
      "file information: UJuan 1 8 HardMult\n",
      "file information: UJing 3 8 HardMult\n",
      "Loading test\n",
      "file information: UJing 1 3 EasyAdd\n",
      "file information: UJing 1 5 EasyAdd\n",
      "file information: UJing 1 9 EasyAdd\n",
      "file information: UJing 3 5 EasyAdd\n",
      "file information: UJing 3 11 EasyAdd\n",
      "file information: UJuan 2 1 EasyAdd\n",
      "file information: UJuan 2 3 EasyAdd\n",
      "file information: UJuan 3 9 EasyAdd\n",
      "file information: UJing 1 4 HardMult\n",
      "file information: UJing 1 6 HardMult\n",
      "file information: UJing 1 10 HardMult\n",
      "file information: UJing 3 6 HardMult\n",
      "file information: UJing 3 12 HardMult\n",
      "file information: UJuan 2 2 HardMult\n",
      "file information: UJuan 2 4 HardMult\n",
      "file information: UJuan 3 10 HardMult\n"
     ]
    }
   ],
   "source": [
    "# x_train, x_test, y_train, y_test = train_test_split(X_arith_seq, y_arith_seq, test_size=0.20,shuffle=True)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X_eyes, y_eyes, test_size=0.20,shuffle=True)\n",
    "\n",
    "x_train, x_test, y_train, y_test = buildTrainAndVal(dataPath,split_rate=0.20, frame_duration= frame_length,overlap=overlap,\n",
    "                                                    lstm_format=True,lstm_sequence_length=sequence_length,lstm_stride=2, sequence_overlap=True,\n",
    "                                                    labels = [\"EasyAdd\", \"HardMult\"], file_name_format=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (1210, 20, 90) (1210,)\n",
      "Test (350, 20, 90) (350,)\n"
     ]
    }
   ],
   "source": [
    "##Normalize the dataset\n",
    "\n",
    "##No normalization -- Seems to yield the best results \n",
    "# x_train_2 = x_train\n",
    "# x_test_2 = x_test\n",
    "\n",
    "##Global frame/time normalization\n",
    "global_mean = x_train.mean(); global_std = x_train.std();\n",
    "x_train_2 = (x_train - global_mean)/global_std;\n",
    "x_test_2 = (x_test - global_mean)/global_std;\n",
    "\n",
    "## convert class vectors to binary class matrices\n",
    "y_train_2 = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_2 = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train_2 = x_train_2.astype('float32')\n",
    "x_test_2 = x_test_2.astype('float32')\n",
    "\n",
    "print(\"Train\", x_train.shape, y_train.shape)\n",
    "print(\"Test\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1210 samples, validate on 350 samples\n",
      "Epoch 1/500\n",
      "1210/1210 [==============================] - 8s 7ms/sample - loss: 4.2724 - accuracy: 0.4860 - val_loss: 4.0817 - val_accuracy: 0.5000\n",
      "Epoch 2/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 3.9439 - accuracy: 0.5231 - val_loss: 3.7621 - val_accuracy: 0.5257\n",
      "Epoch 3/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 3.6336 - accuracy: 0.5711 - val_loss: 3.4681 - val_accuracy: 0.5486\n",
      "Epoch 4/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 3.3538 - accuracy: 0.6140 - val_loss: 3.2011 - val_accuracy: 0.5629\n",
      "Epoch 5/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 3.0982 - accuracy: 0.6471 - val_loss: 2.9566 - val_accuracy: 0.6200\n",
      "Epoch 6/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 2.8612 - accuracy: 0.6488 - val_loss: 2.7328 - val_accuracy: 0.6657\n",
      "Epoch 7/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 2.6441 - accuracy: 0.6983 - val_loss: 2.5289 - val_accuracy: 0.6800\n",
      "Epoch 8/500\n",
      "1210/1210 [==============================] - 0s 150us/sample - loss: 2.4450 - accuracy: 0.7174 - val_loss: 2.3417 - val_accuracy: 0.7000\n",
      "Epoch 9/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 2.2562 - accuracy: 0.7306 - val_loss: 2.1668 - val_accuracy: 0.7171\n",
      "Epoch 10/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 2.0825 - accuracy: 0.7521 - val_loss: 1.9991 - val_accuracy: 0.7343\n",
      "Epoch 11/500\n",
      "1210/1210 [==============================] - 0s 152us/sample - loss: 1.9299 - accuracy: 0.7413 - val_loss: 1.8456 - val_accuracy: 0.7429\n",
      "Epoch 12/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 1.7639 - accuracy: 0.7860 - val_loss: 1.7099 - val_accuracy: 0.7571\n",
      "Epoch 13/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 1.6203 - accuracy: 0.7917 - val_loss: 1.5763 - val_accuracy: 0.7714\n",
      "Epoch 14/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 1.5021 - accuracy: 0.7934 - val_loss: 1.4690 - val_accuracy: 0.7800\n",
      "Epoch 15/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 1.3907 - accuracy: 0.7992 - val_loss: 1.3805 - val_accuracy: 0.7657\n",
      "Epoch 16/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 1.2777 - accuracy: 0.8132 - val_loss: 1.2922 - val_accuracy: 0.7714\n",
      "Epoch 17/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 1.1781 - accuracy: 0.8281 - val_loss: 1.2312 - val_accuracy: 0.7657\n",
      "Epoch 18/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 1.1225 - accuracy: 0.8149 - val_loss: 1.2123 - val_accuracy: 0.7429\n",
      "Epoch 19/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 1.0361 - accuracy: 0.8264 - val_loss: 1.1536 - val_accuracy: 0.7400\n",
      "Epoch 20/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 0.9751 - accuracy: 0.8347 - val_loss: 1.0581 - val_accuracy: 0.7600\n",
      "Epoch 21/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.9009 - accuracy: 0.8529 - val_loss: 1.0553 - val_accuracy: 0.7343\n",
      "Epoch 22/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.8662 - accuracy: 0.8339 - val_loss: 0.9914 - val_accuracy: 0.7457\n",
      "Epoch 23/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.8128 - accuracy: 0.8430 - val_loss: 0.9680 - val_accuracy: 0.7371\n",
      "Epoch 24/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.7587 - accuracy: 0.8579 - val_loss: 0.9339 - val_accuracy: 0.7343\n",
      "Epoch 25/500\n",
      "1210/1210 [==============================] - 0s 150us/sample - loss: 0.7091 - accuracy: 0.8645 - val_loss: 0.8720 - val_accuracy: 0.7714\n",
      "Epoch 26/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.6939 - accuracy: 0.8537 - val_loss: 0.9237 - val_accuracy: 0.7143\n",
      "Epoch 27/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.6568 - accuracy: 0.8587 - val_loss: 0.8378 - val_accuracy: 0.7429\n",
      "Epoch 28/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.5993 - accuracy: 0.8752 - val_loss: 0.8455 - val_accuracy: 0.7314\n",
      "Epoch 29/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.6150 - accuracy: 0.8570 - val_loss: 0.8388 - val_accuracy: 0.7314\n",
      "Epoch 30/500\n",
      "1210/1210 [==============================] - 0s 151us/sample - loss: 0.5796 - accuracy: 0.8612 - val_loss: 0.7899 - val_accuracy: 0.7457\n",
      "Epoch 31/500\n",
      "1210/1210 [==============================] - 0s 154us/sample - loss: 0.5573 - accuracy: 0.8702 - val_loss: 0.7923 - val_accuracy: 0.7371\n",
      "Epoch 32/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.5250 - accuracy: 0.8686 - val_loss: 0.7712 - val_accuracy: 0.7371\n",
      "Epoch 33/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.4914 - accuracy: 0.8826 - val_loss: 0.7715 - val_accuracy: 0.7314\n",
      "Epoch 34/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.4910 - accuracy: 0.8744 - val_loss: 0.7422 - val_accuracy: 0.7371\n",
      "Epoch 35/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.4650 - accuracy: 0.8818 - val_loss: 0.6829 - val_accuracy: 0.7571\n",
      "Epoch 36/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.4591 - accuracy: 0.8744 - val_loss: 0.7300 - val_accuracy: 0.7343\n",
      "Epoch 37/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.4564 - accuracy: 0.8777 - val_loss: 0.6827 - val_accuracy: 0.7457\n",
      "Epoch 38/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.4316 - accuracy: 0.8876 - val_loss: 0.6432 - val_accuracy: 0.7514\n",
      "Epoch 39/500\n",
      "1210/1210 [==============================] - 0s 154us/sample - loss: 0.4099 - accuracy: 0.8893 - val_loss: 0.6807 - val_accuracy: 0.7457\n",
      "Epoch 40/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 0.4174 - accuracy: 0.8752 - val_loss: 0.6634 - val_accuracy: 0.7286\n",
      "Epoch 41/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.3990 - accuracy: 0.8876 - val_loss: 0.6587 - val_accuracy: 0.7486\n",
      "Epoch 42/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.3612 - accuracy: 0.9008 - val_loss: 0.6489 - val_accuracy: 0.7486\n",
      "Epoch 43/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.3848 - accuracy: 0.8802 - val_loss: 0.6665 - val_accuracy: 0.7514\n",
      "Epoch 44/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.3777 - accuracy: 0.8826 - val_loss: 0.6556 - val_accuracy: 0.7629\n",
      "Epoch 45/500\n",
      "1210/1210 [==============================] - 0s 150us/sample - loss: 0.3740 - accuracy: 0.8901 - val_loss: 0.6178 - val_accuracy: 0.7400\n",
      "Epoch 46/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 0.3679 - accuracy: 0.8802 - val_loss: 0.6391 - val_accuracy: 0.7429\n",
      "Epoch 47/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.3350 - accuracy: 0.8992 - val_loss: 0.6367 - val_accuracy: 0.7429\n",
      "Epoch 48/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.3701 - accuracy: 0.8785 - val_loss: 0.6446 - val_accuracy: 0.7457\n",
      "Epoch 49/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.3311 - accuracy: 0.8950 - val_loss: 0.6106 - val_accuracy: 0.7657\n",
      "Epoch 50/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.3369 - accuracy: 0.8843 - val_loss: 0.6197 - val_accuracy: 0.7486\n",
      "Epoch 51/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.3182 - accuracy: 0.9041 - val_loss: 0.5992 - val_accuracy: 0.7343\n",
      "Epoch 52/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.3173 - accuracy: 0.8942 - val_loss: 0.5989 - val_accuracy: 0.7429\n",
      "Epoch 53/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.3378 - accuracy: 0.8893 - val_loss: 0.5968 - val_accuracy: 0.7543\n",
      "Epoch 54/500\n",
      "1210/1210 [==============================] - 0s 150us/sample - loss: 0.3132 - accuracy: 0.8917 - val_loss: 0.6303 - val_accuracy: 0.7543\n",
      "Epoch 55/500\n",
      "1210/1210 [==============================] - 0s 142us/sample - loss: 0.3187 - accuracy: 0.8893 - val_loss: 0.6125 - val_accuracy: 0.7571\n",
      "Epoch 56/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.2965 - accuracy: 0.9074 - val_loss: 0.6120 - val_accuracy: 0.7571\n",
      "Epoch 57/500\n",
      "1210/1210 [==============================] - 0s 142us/sample - loss: 0.2823 - accuracy: 0.9140 - val_loss: 0.5994 - val_accuracy: 0.7714\n",
      "Epoch 58/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.2987 - accuracy: 0.9017 - val_loss: 0.5760 - val_accuracy: 0.7771\n",
      "Epoch 59/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.3155 - accuracy: 0.8917 - val_loss: 0.5703 - val_accuracy: 0.7629\n",
      "Epoch 60/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.2887 - accuracy: 0.9000 - val_loss: 0.5899 - val_accuracy: 0.7657\n",
      "Epoch 61/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2777 - accuracy: 0.9000 - val_loss: 0.5782 - val_accuracy: 0.7486\n",
      "Epoch 62/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.3000 - accuracy: 0.8917 - val_loss: 0.6495 - val_accuracy: 0.7286\n",
      "Epoch 63/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.2869 - accuracy: 0.8942 - val_loss: 0.5686 - val_accuracy: 0.7486\n",
      "Epoch 64/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.3032 - accuracy: 0.8959 - val_loss: 0.6343 - val_accuracy: 0.7486\n",
      "Epoch 65/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2551 - accuracy: 0.9198 - val_loss: 0.5654 - val_accuracy: 0.7743\n",
      "Epoch 66/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.2937 - accuracy: 0.8942 - val_loss: 0.6258 - val_accuracy: 0.7457\n",
      "Epoch 67/500\n",
      "1210/1210 [==============================] - 0s 153us/sample - loss: 0.2958 - accuracy: 0.8868 - val_loss: 0.6210 - val_accuracy: 0.7600\n",
      "Epoch 68/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.2903 - accuracy: 0.8942 - val_loss: 0.5978 - val_accuracy: 0.7571\n",
      "Epoch 69/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.2710 - accuracy: 0.9050 - val_loss: 0.6125 - val_accuracy: 0.7543\n",
      "Epoch 70/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.2753 - accuracy: 0.9066 - val_loss: 0.6210 - val_accuracy: 0.7343\n",
      "Epoch 71/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.2620 - accuracy: 0.9050 - val_loss: 0.6065 - val_accuracy: 0.7486\n",
      "Epoch 72/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.2646 - accuracy: 0.9033 - val_loss: 0.6117 - val_accuracy: 0.7600\n",
      "Epoch 73/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.2591 - accuracy: 0.9066 - val_loss: 0.6473 - val_accuracy: 0.7457\n",
      "Epoch 74/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.2584 - accuracy: 0.9050 - val_loss: 0.6072 - val_accuracy: 0.7571\n",
      "Epoch 75/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.2571 - accuracy: 0.9149 - val_loss: 0.6211 - val_accuracy: 0.7486\n",
      "Epoch 76/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.2719 - accuracy: 0.8975 - val_loss: 0.5982 - val_accuracy: 0.7543\n",
      "Epoch 77/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2260 - accuracy: 0.9314 - val_loss: 0.6444 - val_accuracy: 0.7343\n",
      "Epoch 78/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2685 - accuracy: 0.9017 - val_loss: 0.6044 - val_accuracy: 0.7543\n",
      "Epoch 79/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2390 - accuracy: 0.9116 - val_loss: 0.5731 - val_accuracy: 0.7629\n",
      "Epoch 80/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.2492 - accuracy: 0.9099 - val_loss: 0.5893 - val_accuracy: 0.7629\n",
      "Epoch 81/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.2533 - accuracy: 0.9140 - val_loss: 0.6205 - val_accuracy: 0.7571\n",
      "Epoch 82/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.2322 - accuracy: 0.9107 - val_loss: 0.6131 - val_accuracy: 0.7343\n",
      "Epoch 83/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.2476 - accuracy: 0.9066 - val_loss: 0.6128 - val_accuracy: 0.7657\n",
      "Epoch 84/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.2525 - accuracy: 0.9033 - val_loss: 0.6033 - val_accuracy: 0.7543\n",
      "Epoch 85/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2384 - accuracy: 0.9165 - val_loss: 0.5829 - val_accuracy: 0.7629\n",
      "Epoch 86/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.2631 - accuracy: 0.8950 - val_loss: 0.7039 - val_accuracy: 0.7571\n",
      "Epoch 87/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2409 - accuracy: 0.9132 - val_loss: 0.5362 - val_accuracy: 0.7686\n",
      "Epoch 88/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 0.2412 - accuracy: 0.9099 - val_loss: 0.6265 - val_accuracy: 0.7743\n",
      "Epoch 89/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.2542 - accuracy: 0.9116 - val_loss: 0.5760 - val_accuracy: 0.7629\n",
      "Epoch 90/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.2482 - accuracy: 0.9041 - val_loss: 0.5403 - val_accuracy: 0.7657\n",
      "Epoch 91/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2300 - accuracy: 0.9223 - val_loss: 0.6195 - val_accuracy: 0.7543\n",
      "Epoch 92/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.2621 - accuracy: 0.9099 - val_loss: 0.5932 - val_accuracy: 0.7629\n",
      "Epoch 93/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 0.2430 - accuracy: 0.9140 - val_loss: 0.6231 - val_accuracy: 0.7571\n",
      "Epoch 94/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.2424 - accuracy: 0.9091 - val_loss: 0.5865 - val_accuracy: 0.7686\n",
      "Epoch 95/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.2097 - accuracy: 0.9264 - val_loss: 0.5840 - val_accuracy: 0.7657\n",
      "Epoch 96/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.2311 - accuracy: 0.9174 - val_loss: 0.6324 - val_accuracy: 0.7600\n",
      "Epoch 97/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2181 - accuracy: 0.9248 - val_loss: 0.6112 - val_accuracy: 0.7657\n",
      "Epoch 98/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2302 - accuracy: 0.9223 - val_loss: 0.6791 - val_accuracy: 0.7371\n",
      "Epoch 99/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2442 - accuracy: 0.9107 - val_loss: 0.6058 - val_accuracy: 0.7629\n",
      "Epoch 100/500\n",
      "1210/1210 [==============================] - 0s 150us/sample - loss: 0.2310 - accuracy: 0.9190 - val_loss: 0.6053 - val_accuracy: 0.7686\n",
      "Epoch 101/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 0.2198 - accuracy: 0.9264 - val_loss: 0.6257 - val_accuracy: 0.7714\n",
      "Epoch 102/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 0.2240 - accuracy: 0.9198 - val_loss: 0.6802 - val_accuracy: 0.7514\n",
      "Epoch 103/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 0.2320 - accuracy: 0.9149 - val_loss: 0.5995 - val_accuracy: 0.7657\n",
      "Epoch 104/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.2429 - accuracy: 0.9099 - val_loss: 0.5907 - val_accuracy: 0.7629\n",
      "Epoch 105/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.2211 - accuracy: 0.9223 - val_loss: 0.6711 - val_accuracy: 0.7429\n",
      "Epoch 106/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2068 - accuracy: 0.9372 - val_loss: 0.6438 - val_accuracy: 0.7714\n",
      "Epoch 107/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2164 - accuracy: 0.9165 - val_loss: 0.7272 - val_accuracy: 0.7400\n",
      "Epoch 108/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.2562 - accuracy: 0.9041 - val_loss: 0.6228 - val_accuracy: 0.7686\n",
      "Epoch 109/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.2244 - accuracy: 0.9306 - val_loss: 0.6391 - val_accuracy: 0.7629\n",
      "Epoch 110/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2195 - accuracy: 0.9223 - val_loss: 0.6662 - val_accuracy: 0.7543\n",
      "Epoch 111/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.1977 - accuracy: 0.9273 - val_loss: 0.6737 - val_accuracy: 0.7400\n",
      "Epoch 112/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.2119 - accuracy: 0.9256 - val_loss: 0.7079 - val_accuracy: 0.7343\n",
      "Epoch 113/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2120 - accuracy: 0.9190 - val_loss: 0.6443 - val_accuracy: 0.7486\n",
      "Epoch 114/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.2187 - accuracy: 0.9190 - val_loss: 0.7314 - val_accuracy: 0.7457\n",
      "Epoch 115/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.2349 - accuracy: 0.9132 - val_loss: 0.6319 - val_accuracy: 0.7457\n",
      "Epoch 116/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2095 - accuracy: 0.9190 - val_loss: 0.6483 - val_accuracy: 0.7429\n",
      "Epoch 117/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.2189 - accuracy: 0.9215 - val_loss: 0.6180 - val_accuracy: 0.7629\n",
      "Epoch 118/500\n",
      "1210/1210 [==============================] - 0s 140us/sample - loss: 0.2243 - accuracy: 0.9273 - val_loss: 0.6883 - val_accuracy: 0.7486\n",
      "Epoch 119/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.2141 - accuracy: 0.9264 - val_loss: 0.6330 - val_accuracy: 0.7514\n",
      "Epoch 120/500\n",
      "1210/1210 [==============================] - 0s 142us/sample - loss: 0.1940 - accuracy: 0.9298 - val_loss: 0.7259 - val_accuracy: 0.7543\n",
      "Epoch 121/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.2198 - accuracy: 0.9149 - val_loss: 0.6712 - val_accuracy: 0.7400\n",
      "Epoch 122/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2145 - accuracy: 0.9240 - val_loss: 0.6862 - val_accuracy: 0.7600\n",
      "Epoch 123/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1885 - accuracy: 0.9372 - val_loss: 0.5980 - val_accuracy: 0.7657\n",
      "Epoch 124/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1967 - accuracy: 0.9355 - val_loss: 0.7135 - val_accuracy: 0.7314\n",
      "Epoch 125/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2030 - accuracy: 0.9314 - val_loss: 0.6933 - val_accuracy: 0.7543\n",
      "Epoch 126/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1788 - accuracy: 0.9388 - val_loss: 0.6846 - val_accuracy: 0.7486\n",
      "Epoch 127/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.2111 - accuracy: 0.9223 - val_loss: 0.7091 - val_accuracy: 0.7486\n",
      "Epoch 128/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1994 - accuracy: 0.9273 - val_loss: 0.5825 - val_accuracy: 0.7686\n",
      "Epoch 129/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1818 - accuracy: 0.9322 - val_loss: 0.6527 - val_accuracy: 0.7343\n",
      "Epoch 130/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.2144 - accuracy: 0.9248 - val_loss: 0.6576 - val_accuracy: 0.7486\n",
      "Epoch 131/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1788 - accuracy: 0.9413 - val_loss: 0.6113 - val_accuracy: 0.7514\n",
      "Epoch 132/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.2137 - accuracy: 0.9273 - val_loss: 0.6245 - val_accuracy: 0.7486\n",
      "Epoch 133/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1922 - accuracy: 0.9248 - val_loss: 0.6932 - val_accuracy: 0.7400\n",
      "Epoch 134/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1763 - accuracy: 0.9446 - val_loss: 0.6151 - val_accuracy: 0.7629\n",
      "Epoch 135/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.2119 - accuracy: 0.9240 - val_loss: 0.6755 - val_accuracy: 0.7457\n",
      "Epoch 136/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.2185 - accuracy: 0.9207 - val_loss: 0.6525 - val_accuracy: 0.7486\n",
      "Epoch 137/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1896 - accuracy: 0.9339 - val_loss: 0.5942 - val_accuracy: 0.7714\n",
      "Epoch 138/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.2113 - accuracy: 0.9322 - val_loss: 0.6956 - val_accuracy: 0.7543\n",
      "Epoch 139/500\n",
      "1210/1210 [==============================] - 0s 152us/sample - loss: 0.2004 - accuracy: 0.9289 - val_loss: 0.6761 - val_accuracy: 0.7600\n",
      "Epoch 140/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1852 - accuracy: 0.9314 - val_loss: 0.6547 - val_accuracy: 0.7686\n",
      "Epoch 141/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1995 - accuracy: 0.9314 - val_loss: 0.6781 - val_accuracy: 0.7629\n",
      "Epoch 142/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1827 - accuracy: 0.9438 - val_loss: 0.6852 - val_accuracy: 0.7686\n",
      "Epoch 143/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2137 - accuracy: 0.9215 - val_loss: 0.6090 - val_accuracy: 0.7771\n",
      "Epoch 144/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1928 - accuracy: 0.9314 - val_loss: 0.7145 - val_accuracy: 0.7429\n",
      "Epoch 145/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.2000 - accuracy: 0.9331 - val_loss: 0.5767 - val_accuracy: 0.7686\n",
      "Epoch 146/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1926 - accuracy: 0.9306 - val_loss: 0.6684 - val_accuracy: 0.7457\n",
      "Epoch 147/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1991 - accuracy: 0.9306 - val_loss: 0.6199 - val_accuracy: 0.7457\n",
      "Epoch 148/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1843 - accuracy: 0.9397 - val_loss: 0.7493 - val_accuracy: 0.7400\n",
      "Epoch 149/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1936 - accuracy: 0.9289 - val_loss: 0.6110 - val_accuracy: 0.7543\n",
      "Epoch 150/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1840 - accuracy: 0.9372 - val_loss: 0.6411 - val_accuracy: 0.7486\n",
      "Epoch 151/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1846 - accuracy: 0.9388 - val_loss: 0.6634 - val_accuracy: 0.7600\n",
      "Epoch 152/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1691 - accuracy: 0.9438 - val_loss: 0.6357 - val_accuracy: 0.7571\n",
      "Epoch 153/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1874 - accuracy: 0.9421 - val_loss: 0.6998 - val_accuracy: 0.7571\n",
      "Epoch 154/500\n",
      "1210/1210 [==============================] - 0s 158us/sample - loss: 0.1706 - accuracy: 0.9388 - val_loss: 0.7387 - val_accuracy: 0.7514\n",
      "Epoch 155/500\n",
      "1210/1210 [==============================] - 0s 150us/sample - loss: 0.1709 - accuracy: 0.9364 - val_loss: 0.7317 - val_accuracy: 0.7543\n",
      "Epoch 156/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1847 - accuracy: 0.9380 - val_loss: 0.7415 - val_accuracy: 0.7457\n",
      "Epoch 157/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1709 - accuracy: 0.9397 - val_loss: 0.7211 - val_accuracy: 0.7429\n",
      "Epoch 158/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1600 - accuracy: 0.9521 - val_loss: 0.6482 - val_accuracy: 0.7600\n",
      "Epoch 159/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1656 - accuracy: 0.9438 - val_loss: 0.8317 - val_accuracy: 0.7457\n",
      "Epoch 160/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1947 - accuracy: 0.9289 - val_loss: 0.7259 - val_accuracy: 0.7429\n",
      "Epoch 161/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1652 - accuracy: 0.9446 - val_loss: 0.7167 - val_accuracy: 0.7457\n",
      "Epoch 162/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1764 - accuracy: 0.9314 - val_loss: 0.6856 - val_accuracy: 0.7743\n",
      "Epoch 163/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1822 - accuracy: 0.9364 - val_loss: 0.7335 - val_accuracy: 0.7457\n",
      "Epoch 164/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.1864 - accuracy: 0.9372 - val_loss: 0.6782 - val_accuracy: 0.7743\n",
      "Epoch 165/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1802 - accuracy: 0.9421 - val_loss: 0.6503 - val_accuracy: 0.7600\n",
      "Epoch 166/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1757 - accuracy: 0.9380 - val_loss: 0.6300 - val_accuracy: 0.7771\n",
      "Epoch 167/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1714 - accuracy: 0.9405 - val_loss: 0.7262 - val_accuracy: 0.7629\n",
      "Epoch 168/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1981 - accuracy: 0.9306 - val_loss: 0.6472 - val_accuracy: 0.7657\n",
      "Epoch 169/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1645 - accuracy: 0.9471 - val_loss: 0.6798 - val_accuracy: 0.7486\n",
      "Epoch 170/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1703 - accuracy: 0.9421 - val_loss: 0.6549 - val_accuracy: 0.7514\n",
      "Epoch 171/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1586 - accuracy: 0.9438 - val_loss: 0.7042 - val_accuracy: 0.7514\n",
      "Epoch 172/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1557 - accuracy: 0.9421 - val_loss: 0.7242 - val_accuracy: 0.7543\n",
      "Epoch 173/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1813 - accuracy: 0.9446 - val_loss: 0.7700 - val_accuracy: 0.7429\n",
      "Epoch 174/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1428 - accuracy: 0.9446 - val_loss: 0.7491 - val_accuracy: 0.7629\n",
      "Epoch 175/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1822 - accuracy: 0.9397 - val_loss: 0.6596 - val_accuracy: 0.7600\n",
      "Epoch 176/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1740 - accuracy: 0.9438 - val_loss: 0.6316 - val_accuracy: 0.7657\n",
      "Epoch 177/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1686 - accuracy: 0.9364 - val_loss: 0.6846 - val_accuracy: 0.7657\n",
      "Epoch 178/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1873 - accuracy: 0.9289 - val_loss: 0.6422 - val_accuracy: 0.7571\n",
      "Epoch 179/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1603 - accuracy: 0.9471 - val_loss: 0.6543 - val_accuracy: 0.7600\n",
      "Epoch 180/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1605 - accuracy: 0.9521 - val_loss: 0.6592 - val_accuracy: 0.7657\n",
      "Epoch 181/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1845 - accuracy: 0.9372 - val_loss: 0.6803 - val_accuracy: 0.7629\n",
      "Epoch 182/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1632 - accuracy: 0.9438 - val_loss: 0.7264 - val_accuracy: 0.7486\n",
      "Epoch 183/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1613 - accuracy: 0.9364 - val_loss: 0.7468 - val_accuracy: 0.7514\n",
      "Epoch 184/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1553 - accuracy: 0.9413 - val_loss: 0.8079 - val_accuracy: 0.7400\n",
      "Epoch 185/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1557 - accuracy: 0.9463 - val_loss: 0.8197 - val_accuracy: 0.7429\n",
      "Epoch 186/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1420 - accuracy: 0.9496 - val_loss: 0.8570 - val_accuracy: 0.7429\n",
      "Epoch 187/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1892 - accuracy: 0.9372 - val_loss: 0.7308 - val_accuracy: 0.7457\n",
      "Epoch 188/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1820 - accuracy: 0.9322 - val_loss: 0.6876 - val_accuracy: 0.7543\n",
      "Epoch 189/500\n",
      "1210/1210 [==============================] - 0s 142us/sample - loss: 0.1519 - accuracy: 0.9471 - val_loss: 0.7142 - val_accuracy: 0.7657\n",
      "Epoch 190/500\n",
      "1210/1210 [==============================] - 0s 150us/sample - loss: 0.1864 - accuracy: 0.9347 - val_loss: 0.7864 - val_accuracy: 0.7571\n",
      "Epoch 191/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1905 - accuracy: 0.9298 - val_loss: 0.6879 - val_accuracy: 0.7657\n",
      "Epoch 192/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1506 - accuracy: 0.9471 - val_loss: 0.6945 - val_accuracy: 0.7571\n",
      "Epoch 193/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1728 - accuracy: 0.9430 - val_loss: 0.6939 - val_accuracy: 0.7629\n",
      "Epoch 194/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1654 - accuracy: 0.9421 - val_loss: 0.7593 - val_accuracy: 0.7571\n",
      "Epoch 195/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.1484 - accuracy: 0.9529 - val_loss: 0.7635 - val_accuracy: 0.7600\n",
      "Epoch 196/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1711 - accuracy: 0.9463 - val_loss: 0.7012 - val_accuracy: 0.7686\n",
      "Epoch 197/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1474 - accuracy: 0.9529 - val_loss: 0.8258 - val_accuracy: 0.7514\n",
      "Epoch 198/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1487 - accuracy: 0.9438 - val_loss: 0.7991 - val_accuracy: 0.7600\n",
      "Epoch 199/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1724 - accuracy: 0.9421 - val_loss: 0.7943 - val_accuracy: 0.7486\n",
      "Epoch 200/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1643 - accuracy: 0.9380 - val_loss: 0.6603 - val_accuracy: 0.7571\n",
      "Epoch 201/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1793 - accuracy: 0.9331 - val_loss: 0.8344 - val_accuracy: 0.7429\n",
      "Epoch 202/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1561 - accuracy: 0.9430 - val_loss: 0.7364 - val_accuracy: 0.7571\n",
      "Epoch 203/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1538 - accuracy: 0.9471 - val_loss: 0.6954 - val_accuracy: 0.7743\n",
      "Epoch 204/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1745 - accuracy: 0.9355 - val_loss: 0.8131 - val_accuracy: 0.7514\n",
      "Epoch 205/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1431 - accuracy: 0.9512 - val_loss: 0.8742 - val_accuracy: 0.7486\n",
      "Epoch 206/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1671 - accuracy: 0.9438 - val_loss: 0.7233 - val_accuracy: 0.7571\n",
      "Epoch 207/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1534 - accuracy: 0.9455 - val_loss: 0.7541 - val_accuracy: 0.7629\n",
      "Epoch 208/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1503 - accuracy: 0.9496 - val_loss: 0.6931 - val_accuracy: 0.7600\n",
      "Epoch 209/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1753 - accuracy: 0.9405 - val_loss: 0.6513 - val_accuracy: 0.7657\n",
      "Epoch 210/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1427 - accuracy: 0.9504 - val_loss: 0.7165 - val_accuracy: 0.7514\n",
      "Epoch 211/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1708 - accuracy: 0.9413 - val_loss: 0.7198 - val_accuracy: 0.7571\n",
      "Epoch 212/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1601 - accuracy: 0.9438 - val_loss: 0.7573 - val_accuracy: 0.7486\n",
      "Epoch 213/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1514 - accuracy: 0.9496 - val_loss: 0.6683 - val_accuracy: 0.7600\n",
      "Epoch 214/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1470 - accuracy: 0.9521 - val_loss: 0.7642 - val_accuracy: 0.7543\n",
      "Epoch 215/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1707 - accuracy: 0.9413 - val_loss: 0.7380 - val_accuracy: 0.7657\n",
      "Epoch 216/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1607 - accuracy: 0.9537 - val_loss: 0.7619 - val_accuracy: 0.7457\n",
      "Epoch 217/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1538 - accuracy: 0.9455 - val_loss: 0.7307 - val_accuracy: 0.7429\n",
      "Epoch 218/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1746 - accuracy: 0.9388 - val_loss: 0.7467 - val_accuracy: 0.7543\n",
      "Epoch 219/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1419 - accuracy: 0.9529 - val_loss: 0.6708 - val_accuracy: 0.7543\n",
      "Epoch 220/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1440 - accuracy: 0.9521 - val_loss: 0.7777 - val_accuracy: 0.7543\n",
      "Epoch 221/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1759 - accuracy: 0.9438 - val_loss: 0.7686 - val_accuracy: 0.7629\n",
      "Epoch 222/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1484 - accuracy: 0.9446 - val_loss: 0.7145 - val_accuracy: 0.7571\n",
      "Epoch 223/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1521 - accuracy: 0.9488 - val_loss: 0.7376 - val_accuracy: 0.7571\n",
      "Epoch 224/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1631 - accuracy: 0.9446 - val_loss: 0.7971 - val_accuracy: 0.7429\n",
      "Epoch 225/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1762 - accuracy: 0.9405 - val_loss: 0.6817 - val_accuracy: 0.7600\n",
      "Epoch 226/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1821 - accuracy: 0.9347 - val_loss: 0.6370 - val_accuracy: 0.7571\n",
      "Epoch 227/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1810 - accuracy: 0.9339 - val_loss: 0.7879 - val_accuracy: 0.7371\n",
      "Epoch 228/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1494 - accuracy: 0.9504 - val_loss: 0.6367 - val_accuracy: 0.7600\n",
      "Epoch 229/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1598 - accuracy: 0.9397 - val_loss: 0.7095 - val_accuracy: 0.7686\n",
      "Epoch 230/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1503 - accuracy: 0.9488 - val_loss: 0.9076 - val_accuracy: 0.7371\n",
      "Epoch 231/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1699 - accuracy: 0.9446 - val_loss: 0.8203 - val_accuracy: 0.7486\n",
      "Epoch 232/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1406 - accuracy: 0.9562 - val_loss: 0.8412 - val_accuracy: 0.7457\n",
      "Epoch 233/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1558 - accuracy: 0.9496 - val_loss: 0.7610 - val_accuracy: 0.7571\n",
      "Epoch 234/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1623 - accuracy: 0.9446 - val_loss: 0.7485 - val_accuracy: 0.7514\n",
      "Epoch 235/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1477 - accuracy: 0.9463 - val_loss: 0.7506 - val_accuracy: 0.7514\n",
      "Epoch 236/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1588 - accuracy: 0.9471 - val_loss: 0.6809 - val_accuracy: 0.7771\n",
      "Epoch 237/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1550 - accuracy: 0.9463 - val_loss: 0.8204 - val_accuracy: 0.7514\n",
      "Epoch 238/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1454 - accuracy: 0.9537 - val_loss: 0.7905 - val_accuracy: 0.7571\n",
      "Epoch 239/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1676 - accuracy: 0.9380 - val_loss: 0.7164 - val_accuracy: 0.7543\n",
      "Epoch 240/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1389 - accuracy: 0.9545 - val_loss: 0.7211 - val_accuracy: 0.7543\n",
      "Epoch 241/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1660 - accuracy: 0.9413 - val_loss: 0.8055 - val_accuracy: 0.7429\n",
      "Epoch 242/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1426 - accuracy: 0.9529 - val_loss: 0.6624 - val_accuracy: 0.7543\n",
      "Epoch 243/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1530 - accuracy: 0.9488 - val_loss: 0.7336 - val_accuracy: 0.7514\n",
      "Epoch 244/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1475 - accuracy: 0.9463 - val_loss: 0.7725 - val_accuracy: 0.7429\n",
      "Epoch 245/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1452 - accuracy: 0.9529 - val_loss: 0.7664 - val_accuracy: 0.7457\n",
      "Epoch 246/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1599 - accuracy: 0.9397 - val_loss: 0.7564 - val_accuracy: 0.7514\n",
      "Epoch 247/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1480 - accuracy: 0.9562 - val_loss: 0.7938 - val_accuracy: 0.7314\n",
      "Epoch 248/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1677 - accuracy: 0.9446 - val_loss: 0.6875 - val_accuracy: 0.7543\n",
      "Epoch 249/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1655 - accuracy: 0.9413 - val_loss: 0.7991 - val_accuracy: 0.7343\n",
      "Epoch 250/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1524 - accuracy: 0.9421 - val_loss: 0.6571 - val_accuracy: 0.7686\n",
      "Epoch 251/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1674 - accuracy: 0.9397 - val_loss: 0.8335 - val_accuracy: 0.7429\n",
      "Epoch 252/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1273 - accuracy: 0.9595 - val_loss: 0.7842 - val_accuracy: 0.7571\n",
      "Epoch 253/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1413 - accuracy: 0.9554 - val_loss: 0.8237 - val_accuracy: 0.7543\n",
      "Epoch 254/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1579 - accuracy: 0.9455 - val_loss: 0.7157 - val_accuracy: 0.7600\n",
      "Epoch 255/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1480 - accuracy: 0.9471 - val_loss: 0.8310 - val_accuracy: 0.7429\n",
      "Epoch 256/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1613 - accuracy: 0.9388 - val_loss: 0.7360 - val_accuracy: 0.7514\n",
      "Epoch 257/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1444 - accuracy: 0.9488 - val_loss: 0.6920 - val_accuracy: 0.7514\n",
      "Epoch 258/500\n",
      "1210/1210 [==============================] - 0s 152us/sample - loss: 0.1429 - accuracy: 0.9488 - val_loss: 0.8225 - val_accuracy: 0.7457\n",
      "Epoch 259/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1496 - accuracy: 0.9504 - val_loss: 0.7255 - val_accuracy: 0.7543\n",
      "Epoch 260/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1457 - accuracy: 0.9488 - val_loss: 0.8024 - val_accuracy: 0.7400\n",
      "Epoch 261/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1554 - accuracy: 0.9430 - val_loss: 0.7512 - val_accuracy: 0.7543\n",
      "Epoch 262/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1512 - accuracy: 0.9479 - val_loss: 0.8054 - val_accuracy: 0.7571\n",
      "Epoch 263/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1552 - accuracy: 0.9463 - val_loss: 0.7468 - val_accuracy: 0.7543\n",
      "Epoch 264/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1417 - accuracy: 0.9529 - val_loss: 0.7527 - val_accuracy: 0.7543\n",
      "Epoch 265/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1374 - accuracy: 0.9479 - val_loss: 0.8466 - val_accuracy: 0.7371\n",
      "Epoch 266/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1461 - accuracy: 0.9529 - val_loss: 0.8646 - val_accuracy: 0.7429\n",
      "Epoch 267/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1438 - accuracy: 0.9579 - val_loss: 0.8137 - val_accuracy: 0.7343\n",
      "Epoch 268/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1621 - accuracy: 0.9471 - val_loss: 0.6835 - val_accuracy: 0.7514\n",
      "Epoch 269/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1594 - accuracy: 0.9405 - val_loss: 0.8841 - val_accuracy: 0.7400\n",
      "Epoch 270/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1452 - accuracy: 0.9504 - val_loss: 0.7046 - val_accuracy: 0.7686\n",
      "Epoch 271/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1624 - accuracy: 0.9421 - val_loss: 0.7556 - val_accuracy: 0.7543\n",
      "Epoch 272/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1386 - accuracy: 0.9570 - val_loss: 0.7574 - val_accuracy: 0.7486\n",
      "Epoch 273/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1363 - accuracy: 0.9579 - val_loss: 0.6849 - val_accuracy: 0.7657\n",
      "Epoch 274/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1264 - accuracy: 0.9529 - val_loss: 0.8175 - val_accuracy: 0.7486\n",
      "Epoch 275/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1733 - accuracy: 0.9438 - val_loss: 0.7540 - val_accuracy: 0.7571\n",
      "Epoch 276/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1444 - accuracy: 0.9479 - val_loss: 0.8552 - val_accuracy: 0.7429\n",
      "Epoch 277/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1374 - accuracy: 0.9595 - val_loss: 0.8318 - val_accuracy: 0.7629\n",
      "Epoch 278/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1412 - accuracy: 0.9496 - val_loss: 0.7630 - val_accuracy: 0.7657\n",
      "Epoch 279/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1410 - accuracy: 0.9554 - val_loss: 0.7885 - val_accuracy: 0.7486\n",
      "Epoch 280/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1472 - accuracy: 0.9512 - val_loss: 0.8313 - val_accuracy: 0.7371\n",
      "Epoch 281/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1282 - accuracy: 0.9529 - val_loss: 0.8527 - val_accuracy: 0.7429\n",
      "Epoch 282/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1353 - accuracy: 0.9570 - val_loss: 0.7126 - val_accuracy: 0.7771\n",
      "Epoch 283/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1346 - accuracy: 0.9562 - val_loss: 0.9019 - val_accuracy: 0.7457\n",
      "Epoch 284/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1478 - accuracy: 0.9537 - val_loss: 0.6972 - val_accuracy: 0.7686\n",
      "Epoch 285/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1347 - accuracy: 0.9562 - val_loss: 0.7593 - val_accuracy: 0.7514\n",
      "Epoch 286/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1429 - accuracy: 0.9562 - val_loss: 0.8108 - val_accuracy: 0.7486\n",
      "Epoch 287/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1235 - accuracy: 0.9612 - val_loss: 0.7191 - val_accuracy: 0.7657\n",
      "Epoch 288/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1415 - accuracy: 0.9545 - val_loss: 0.8709 - val_accuracy: 0.7343\n",
      "Epoch 289/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1603 - accuracy: 0.9479 - val_loss: 0.7117 - val_accuracy: 0.7629\n",
      "Epoch 290/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1499 - accuracy: 0.9537 - val_loss: 0.8250 - val_accuracy: 0.7400\n",
      "Epoch 291/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1380 - accuracy: 0.9521 - val_loss: 0.6827 - val_accuracy: 0.7657\n",
      "Epoch 292/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1400 - accuracy: 0.9512 - val_loss: 0.8523 - val_accuracy: 0.7400\n",
      "Epoch 293/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1274 - accuracy: 0.9620 - val_loss: 0.7597 - val_accuracy: 0.7600\n",
      "Epoch 294/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1369 - accuracy: 0.9554 - val_loss: 0.7600 - val_accuracy: 0.7571\n",
      "Epoch 295/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1377 - accuracy: 0.9545 - val_loss: 0.7574 - val_accuracy: 0.7543\n",
      "Epoch 296/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1298 - accuracy: 0.9562 - val_loss: 0.7149 - val_accuracy: 0.7657\n",
      "Epoch 297/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1369 - accuracy: 0.9554 - val_loss: 0.7732 - val_accuracy: 0.7514\n",
      "Epoch 298/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1857 - accuracy: 0.9306 - val_loss: 0.6851 - val_accuracy: 0.7800\n",
      "Epoch 299/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1615 - accuracy: 0.9438 - val_loss: 0.7522 - val_accuracy: 0.7371\n",
      "Epoch 300/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1414 - accuracy: 0.9529 - val_loss: 0.6658 - val_accuracy: 0.7714\n",
      "Epoch 301/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1553 - accuracy: 0.9380 - val_loss: 0.7431 - val_accuracy: 0.7657\n",
      "Epoch 302/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1396 - accuracy: 0.9579 - val_loss: 0.6865 - val_accuracy: 0.7743\n",
      "Epoch 303/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1441 - accuracy: 0.9463 - val_loss: 0.7375 - val_accuracy: 0.7543\n",
      "Epoch 304/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1279 - accuracy: 0.9504 - val_loss: 0.7013 - val_accuracy: 0.7657\n",
      "Epoch 305/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1330 - accuracy: 0.9579 - val_loss: 0.8446 - val_accuracy: 0.7343\n",
      "Epoch 306/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1332 - accuracy: 0.9529 - val_loss: 0.7014 - val_accuracy: 0.7771\n",
      "Epoch 307/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1310 - accuracy: 0.9554 - val_loss: 0.8077 - val_accuracy: 0.7514\n",
      "Epoch 308/500\n",
      "1210/1210 [==============================] - 0s 150us/sample - loss: 0.1375 - accuracy: 0.9479 - val_loss: 0.8196 - val_accuracy: 0.7457\n",
      "Epoch 309/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1279 - accuracy: 0.9587 - val_loss: 0.8769 - val_accuracy: 0.7400\n",
      "Epoch 310/500\n",
      "1210/1210 [==============================] - 0s 151us/sample - loss: 0.1346 - accuracy: 0.9570 - val_loss: 0.7537 - val_accuracy: 0.7629\n",
      "Epoch 311/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1387 - accuracy: 0.9545 - val_loss: 0.8608 - val_accuracy: 0.7429\n",
      "Epoch 312/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1290 - accuracy: 0.9570 - val_loss: 0.8170 - val_accuracy: 0.7600\n",
      "Epoch 313/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1155 - accuracy: 0.9694 - val_loss: 0.8732 - val_accuracy: 0.7486\n",
      "Epoch 314/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1271 - accuracy: 0.9603 - val_loss: 0.9254 - val_accuracy: 0.7429\n",
      "Epoch 315/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1318 - accuracy: 0.9521 - val_loss: 0.9104 - val_accuracy: 0.7286\n",
      "Epoch 316/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1381 - accuracy: 0.9496 - val_loss: 0.8761 - val_accuracy: 0.7286\n",
      "Epoch 317/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1352 - accuracy: 0.9496 - val_loss: 0.7767 - val_accuracy: 0.7600\n",
      "Epoch 318/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1380 - accuracy: 0.9504 - val_loss: 0.8894 - val_accuracy: 0.7343\n",
      "Epoch 319/500\n",
      "1210/1210 [==============================] - 0s 142us/sample - loss: 0.1360 - accuracy: 0.9545 - val_loss: 0.7517 - val_accuracy: 0.7629\n",
      "Epoch 320/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1177 - accuracy: 0.9603 - val_loss: 0.8918 - val_accuracy: 0.7371\n",
      "Epoch 321/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1238 - accuracy: 0.9612 - val_loss: 0.8155 - val_accuracy: 0.7629\n",
      "Epoch 322/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.1190 - accuracy: 0.9645 - val_loss: 0.7774 - val_accuracy: 0.7629\n",
      "Epoch 323/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1407 - accuracy: 0.9479 - val_loss: 0.7545 - val_accuracy: 0.7571\n",
      "Epoch 324/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1481 - accuracy: 0.9512 - val_loss: 0.7215 - val_accuracy: 0.7800\n",
      "Epoch 325/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1311 - accuracy: 0.9562 - val_loss: 0.6876 - val_accuracy: 0.7743\n",
      "Epoch 326/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1405 - accuracy: 0.9537 - val_loss: 0.7584 - val_accuracy: 0.7571\n",
      "Epoch 327/500\n",
      "1210/1210 [==============================] - 0s 141us/sample - loss: 0.1255 - accuracy: 0.9678 - val_loss: 0.7559 - val_accuracy: 0.7571\n",
      "Epoch 328/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 0.1176 - accuracy: 0.9603 - val_loss: 0.7463 - val_accuracy: 0.7686\n",
      "Epoch 329/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1300 - accuracy: 0.9587 - val_loss: 0.7796 - val_accuracy: 0.7486\n",
      "Epoch 330/500\n",
      "1210/1210 [==============================] - 0s 142us/sample - loss: 0.1440 - accuracy: 0.9463 - val_loss: 0.7270 - val_accuracy: 0.7571\n",
      "Epoch 331/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1283 - accuracy: 0.9554 - val_loss: 0.7536 - val_accuracy: 0.7457\n",
      "Epoch 332/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1253 - accuracy: 0.9579 - val_loss: 0.7629 - val_accuracy: 0.7629\n",
      "Epoch 333/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1442 - accuracy: 0.9521 - val_loss: 0.7378 - val_accuracy: 0.7600\n",
      "Epoch 334/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1500 - accuracy: 0.9504 - val_loss: 0.7246 - val_accuracy: 0.7629\n",
      "Epoch 335/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1241 - accuracy: 0.9587 - val_loss: 0.7374 - val_accuracy: 0.7600\n",
      "Epoch 336/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1287 - accuracy: 0.9587 - val_loss: 0.8239 - val_accuracy: 0.7514\n",
      "Epoch 337/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1605 - accuracy: 0.9455 - val_loss: 0.7633 - val_accuracy: 0.7714\n",
      "Epoch 338/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1380 - accuracy: 0.9504 - val_loss: 0.8429 - val_accuracy: 0.7486\n",
      "Epoch 339/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1279 - accuracy: 0.9612 - val_loss: 0.8714 - val_accuracy: 0.7486\n",
      "Epoch 340/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1263 - accuracy: 0.9628 - val_loss: 0.9097 - val_accuracy: 0.7429\n",
      "Epoch 341/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1273 - accuracy: 0.9562 - val_loss: 0.9172 - val_accuracy: 0.7429\n",
      "Epoch 342/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1590 - accuracy: 0.9463 - val_loss: 0.7626 - val_accuracy: 0.7543\n",
      "Epoch 343/500\n",
      "1210/1210 [==============================] - 0s 142us/sample - loss: 0.1338 - accuracy: 0.9570 - val_loss: 0.7594 - val_accuracy: 0.7486\n",
      "Epoch 344/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1341 - accuracy: 0.9554 - val_loss: 0.6788 - val_accuracy: 0.7657\n",
      "Epoch 345/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1447 - accuracy: 0.9521 - val_loss: 0.8098 - val_accuracy: 0.7600\n",
      "Epoch 346/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1350 - accuracy: 0.9554 - val_loss: 0.7948 - val_accuracy: 0.7543\n",
      "Epoch 347/500\n",
      "1210/1210 [==============================] - 0s 142us/sample - loss: 0.1041 - accuracy: 0.9653 - val_loss: 0.8098 - val_accuracy: 0.7600\n",
      "Epoch 348/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1222 - accuracy: 0.9587 - val_loss: 0.9032 - val_accuracy: 0.7457\n",
      "Epoch 349/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1229 - accuracy: 0.9529 - val_loss: 0.8875 - val_accuracy: 0.7571\n",
      "Epoch 350/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1425 - accuracy: 0.9529 - val_loss: 0.8515 - val_accuracy: 0.7514\n",
      "Epoch 351/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1385 - accuracy: 0.9562 - val_loss: 0.9056 - val_accuracy: 0.7429\n",
      "Epoch 352/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1296 - accuracy: 0.9579 - val_loss: 0.7815 - val_accuracy: 0.7714\n",
      "Epoch 353/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1328 - accuracy: 0.9595 - val_loss: 0.7641 - val_accuracy: 0.7543\n",
      "Epoch 354/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1637 - accuracy: 0.9421 - val_loss: 0.7916 - val_accuracy: 0.7657\n",
      "Epoch 355/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1463 - accuracy: 0.9562 - val_loss: 0.6972 - val_accuracy: 0.7629\n",
      "Epoch 356/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1372 - accuracy: 0.9512 - val_loss: 0.6948 - val_accuracy: 0.7629\n",
      "Epoch 357/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1267 - accuracy: 0.9570 - val_loss: 0.7579 - val_accuracy: 0.7543\n",
      "Epoch 358/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1360 - accuracy: 0.9603 - val_loss: 0.8292 - val_accuracy: 0.7371\n",
      "Epoch 359/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1391 - accuracy: 0.9463 - val_loss: 0.7205 - val_accuracy: 0.7543\n",
      "Epoch 360/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1444 - accuracy: 0.9521 - val_loss: 0.7793 - val_accuracy: 0.7514\n",
      "Epoch 361/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1380 - accuracy: 0.9496 - val_loss: 0.7224 - val_accuracy: 0.7571\n",
      "Epoch 362/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1345 - accuracy: 0.9537 - val_loss: 0.8356 - val_accuracy: 0.7543\n",
      "Epoch 363/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1302 - accuracy: 0.9645 - val_loss: 0.7523 - val_accuracy: 0.7571\n",
      "Epoch 364/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1429 - accuracy: 0.9463 - val_loss: 0.8444 - val_accuracy: 0.7400\n",
      "Epoch 365/500\n",
      "1210/1210 [==============================] - 0s 153us/sample - loss: 0.1275 - accuracy: 0.9496 - val_loss: 0.7732 - val_accuracy: 0.7571\n",
      "Epoch 366/500\n",
      "1210/1210 [==============================] - 0s 156us/sample - loss: 0.1168 - accuracy: 0.9653 - val_loss: 0.7652 - val_accuracy: 0.7771\n",
      "Epoch 367/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1151 - accuracy: 0.9620 - val_loss: 0.8708 - val_accuracy: 0.7457\n",
      "Epoch 368/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1039 - accuracy: 0.9636 - val_loss: 0.8461 - val_accuracy: 0.7657\n",
      "Epoch 369/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1099 - accuracy: 0.9636 - val_loss: 0.9560 - val_accuracy: 0.7457\n",
      "Epoch 370/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1575 - accuracy: 0.9496 - val_loss: 0.7490 - val_accuracy: 0.7714\n",
      "Epoch 371/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1410 - accuracy: 0.9562 - val_loss: 0.6889 - val_accuracy: 0.7714\n",
      "Epoch 372/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1274 - accuracy: 0.9620 - val_loss: 0.6491 - val_accuracy: 0.7829\n",
      "Epoch 373/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1327 - accuracy: 0.9562 - val_loss: 0.6867 - val_accuracy: 0.7686\n",
      "Epoch 374/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1209 - accuracy: 0.9612 - val_loss: 0.7090 - val_accuracy: 0.7714\n",
      "Epoch 375/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1291 - accuracy: 0.9628 - val_loss: 0.7106 - val_accuracy: 0.7743\n",
      "Epoch 376/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1238 - accuracy: 0.9579 - val_loss: 0.7767 - val_accuracy: 0.7629\n",
      "Epoch 377/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1152 - accuracy: 0.9653 - val_loss: 0.6738 - val_accuracy: 0.7800\n",
      "Epoch 378/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1233 - accuracy: 0.9603 - val_loss: 0.8022 - val_accuracy: 0.7714\n",
      "Epoch 379/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1519 - accuracy: 0.9545 - val_loss: 0.8064 - val_accuracy: 0.7600\n",
      "Epoch 380/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1250 - accuracy: 0.9612 - val_loss: 0.7297 - val_accuracy: 0.7714\n",
      "Epoch 381/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.1212 - accuracy: 0.9570 - val_loss: 0.7706 - val_accuracy: 0.7657\n",
      "Epoch 382/500\n",
      "1210/1210 [==============================] - 0s 152us/sample - loss: 0.1053 - accuracy: 0.9686 - val_loss: 0.7711 - val_accuracy: 0.7629\n",
      "Epoch 383/500\n",
      "1210/1210 [==============================] - 0s 152us/sample - loss: 0.1199 - accuracy: 0.9595 - val_loss: 0.8127 - val_accuracy: 0.7543\n",
      "Epoch 384/500\n",
      "1210/1210 [==============================] - 0s 152us/sample - loss: 0.1298 - accuracy: 0.9579 - val_loss: 0.7972 - val_accuracy: 0.7657\n",
      "Epoch 385/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1448 - accuracy: 0.9537 - val_loss: 0.8498 - val_accuracy: 0.7571\n",
      "Epoch 386/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1223 - accuracy: 0.9620 - val_loss: 0.7563 - val_accuracy: 0.7657\n",
      "Epoch 387/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1298 - accuracy: 0.9579 - val_loss: 0.8546 - val_accuracy: 0.7400\n",
      "Epoch 388/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 0.1405 - accuracy: 0.9529 - val_loss: 0.7536 - val_accuracy: 0.7657\n",
      "Epoch 389/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.1397 - accuracy: 0.9545 - val_loss: 0.7661 - val_accuracy: 0.7514\n",
      "Epoch 390/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1143 - accuracy: 0.9653 - val_loss: 0.8485 - val_accuracy: 0.7514\n",
      "Epoch 391/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1086 - accuracy: 0.9661 - val_loss: 0.8582 - val_accuracy: 0.7543\n",
      "Epoch 392/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1408 - accuracy: 0.9529 - val_loss: 0.8728 - val_accuracy: 0.7571\n",
      "Epoch 393/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1297 - accuracy: 0.9603 - val_loss: 0.9323 - val_accuracy: 0.7514\n",
      "Epoch 394/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1039 - accuracy: 0.9678 - val_loss: 0.9271 - val_accuracy: 0.7429\n",
      "Epoch 395/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1152 - accuracy: 0.9579 - val_loss: 0.8549 - val_accuracy: 0.7571\n",
      "Epoch 396/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1144 - accuracy: 0.9645 - val_loss: 0.8986 - val_accuracy: 0.7457\n",
      "Epoch 397/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1408 - accuracy: 0.9521 - val_loss: 0.8316 - val_accuracy: 0.7571\n",
      "Epoch 398/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.1327 - accuracy: 0.9620 - val_loss: 0.8596 - val_accuracy: 0.7371\n",
      "Epoch 399/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1326 - accuracy: 0.9529 - val_loss: 0.8168 - val_accuracy: 0.7486\n",
      "Epoch 400/500\n",
      "1210/1210 [==============================] - 0s 150us/sample - loss: 0.1200 - accuracy: 0.9603 - val_loss: 0.8076 - val_accuracy: 0.7686\n",
      "Epoch 401/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1142 - accuracy: 0.9628 - val_loss: 0.9537 - val_accuracy: 0.7486\n",
      "Epoch 402/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1267 - accuracy: 0.9579 - val_loss: 0.7892 - val_accuracy: 0.7600\n",
      "Epoch 403/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1274 - accuracy: 0.9603 - val_loss: 0.7889 - val_accuracy: 0.7486\n",
      "Epoch 404/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1188 - accuracy: 0.9628 - val_loss: 0.7540 - val_accuracy: 0.7543\n",
      "Epoch 405/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1039 - accuracy: 0.9702 - val_loss: 0.7844 - val_accuracy: 0.7600\n",
      "Epoch 406/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1154 - accuracy: 0.9628 - val_loss: 0.7824 - val_accuracy: 0.7600\n",
      "Epoch 407/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1297 - accuracy: 0.9612 - val_loss: 0.8515 - val_accuracy: 0.7571\n",
      "Epoch 408/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 0.1049 - accuracy: 0.9678 - val_loss: 0.8224 - val_accuracy: 0.7571\n",
      "Epoch 409/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1123 - accuracy: 0.9612 - val_loss: 0.8269 - val_accuracy: 0.7571\n",
      "Epoch 410/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1155 - accuracy: 0.9661 - val_loss: 0.8891 - val_accuracy: 0.7543\n",
      "Epoch 411/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1169 - accuracy: 0.9603 - val_loss: 0.7917 - val_accuracy: 0.7657\n",
      "Epoch 412/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1316 - accuracy: 0.9562 - val_loss: 0.7176 - val_accuracy: 0.7686\n",
      "Epoch 413/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1174 - accuracy: 0.9612 - val_loss: 0.8257 - val_accuracy: 0.7714\n",
      "Epoch 414/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 0.1352 - accuracy: 0.9579 - val_loss: 0.7624 - val_accuracy: 0.7743\n",
      "Epoch 415/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1135 - accuracy: 0.9595 - val_loss: 0.7689 - val_accuracy: 0.7514\n",
      "Epoch 416/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.1162 - accuracy: 0.9579 - val_loss: 0.7803 - val_accuracy: 0.7686\n",
      "Epoch 417/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1183 - accuracy: 0.9603 - val_loss: 0.8502 - val_accuracy: 0.7686\n",
      "Epoch 418/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1419 - accuracy: 0.9521 - val_loss: 0.6948 - val_accuracy: 0.7857\n",
      "Epoch 419/500\n",
      "1210/1210 [==============================] - 0s 151us/sample - loss: 0.1496 - accuracy: 0.9529 - val_loss: 0.7974 - val_accuracy: 0.7514\n",
      "Epoch 420/500\n",
      "1210/1210 [==============================] - 0s 150us/sample - loss: 0.1246 - accuracy: 0.9595 - val_loss: 0.7130 - val_accuracy: 0.7657\n",
      "Epoch 421/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1296 - accuracy: 0.9545 - val_loss: 0.7866 - val_accuracy: 0.7629\n",
      "Epoch 422/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.1243 - accuracy: 0.9603 - val_loss: 0.8437 - val_accuracy: 0.7457\n",
      "Epoch 423/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1216 - accuracy: 0.9653 - val_loss: 0.7823 - val_accuracy: 0.7457\n",
      "Epoch 424/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1116 - accuracy: 0.9645 - val_loss: 0.8284 - val_accuracy: 0.7629\n",
      "Epoch 425/500\n",
      "1210/1210 [==============================] - 0s 152us/sample - loss: 0.1043 - accuracy: 0.9702 - val_loss: 0.7829 - val_accuracy: 0.7600\n",
      "Epoch 426/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1334 - accuracy: 0.9554 - val_loss: 0.7667 - val_accuracy: 0.7514\n",
      "Epoch 427/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1201 - accuracy: 0.9661 - val_loss: 0.7788 - val_accuracy: 0.7371\n",
      "Epoch 428/500\n",
      "1210/1210 [==============================] - 0s 151us/sample - loss: 0.1047 - accuracy: 0.9678 - val_loss: 0.8464 - val_accuracy: 0.7457\n",
      "Epoch 429/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1215 - accuracy: 0.9620 - val_loss: 0.8687 - val_accuracy: 0.7400\n",
      "Epoch 430/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1305 - accuracy: 0.9562 - val_loss: 0.7919 - val_accuracy: 0.7600\n",
      "Epoch 431/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1145 - accuracy: 0.9661 - val_loss: 0.7904 - val_accuracy: 0.7657\n",
      "Epoch 432/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.1191 - accuracy: 0.9595 - val_loss: 0.7256 - val_accuracy: 0.7714\n",
      "Epoch 433/500\n",
      "1210/1210 [==============================] - 0s 150us/sample - loss: 0.1477 - accuracy: 0.9504 - val_loss: 0.7734 - val_accuracy: 0.7543\n",
      "Epoch 434/500\n",
      "1210/1210 [==============================] - 0s 151us/sample - loss: 0.1410 - accuracy: 0.9488 - val_loss: 0.6657 - val_accuracy: 0.7886\n",
      "Epoch 435/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1201 - accuracy: 0.9570 - val_loss: 0.7684 - val_accuracy: 0.7600\n",
      "Epoch 436/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1275 - accuracy: 0.9570 - val_loss: 0.7492 - val_accuracy: 0.7686\n",
      "Epoch 437/500\n",
      "1210/1210 [==============================] - 0s 152us/sample - loss: 0.1290 - accuracy: 0.9562 - val_loss: 0.7765 - val_accuracy: 0.7629\n",
      "Epoch 438/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1145 - accuracy: 0.9620 - val_loss: 0.7178 - val_accuracy: 0.7629\n",
      "Epoch 439/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1192 - accuracy: 0.9620 - val_loss: 0.7239 - val_accuracy: 0.7686\n",
      "Epoch 440/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1135 - accuracy: 0.9711 - val_loss: 0.7667 - val_accuracy: 0.7629\n",
      "Epoch 441/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1108 - accuracy: 0.9612 - val_loss: 0.6748 - val_accuracy: 0.7771\n",
      "Epoch 442/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1045 - accuracy: 0.9694 - val_loss: 0.7570 - val_accuracy: 0.7686\n",
      "Epoch 443/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1094 - accuracy: 0.9628 - val_loss: 0.7709 - val_accuracy: 0.7714\n",
      "Epoch 444/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.0993 - accuracy: 0.9719 - val_loss: 0.7353 - val_accuracy: 0.7600\n",
      "Epoch 445/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1183 - accuracy: 0.9612 - val_loss: 0.7831 - val_accuracy: 0.7571\n",
      "Epoch 446/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.1025 - accuracy: 0.9653 - val_loss: 0.7350 - val_accuracy: 0.7600\n",
      "Epoch 447/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1240 - accuracy: 0.9628 - val_loss: 0.7427 - val_accuracy: 0.7657\n",
      "Epoch 448/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1250 - accuracy: 0.9645 - val_loss: 0.8101 - val_accuracy: 0.7629\n",
      "Epoch 449/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1251 - accuracy: 0.9653 - val_loss: 0.7979 - val_accuracy: 0.7543\n",
      "Epoch 450/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.1238 - accuracy: 0.9521 - val_loss: 0.7056 - val_accuracy: 0.7657\n",
      "Epoch 451/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1077 - accuracy: 0.9686 - val_loss: 0.7356 - val_accuracy: 0.7629\n",
      "Epoch 452/500\n",
      "1210/1210 [==============================] - 0s 143us/sample - loss: 0.0952 - accuracy: 0.9702 - val_loss: 0.7462 - val_accuracy: 0.7629\n",
      "Epoch 453/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1047 - accuracy: 0.9669 - val_loss: 0.7793 - val_accuracy: 0.7629\n",
      "Epoch 454/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.0902 - accuracy: 0.9727 - val_loss: 0.8965 - val_accuracy: 0.7400\n",
      "Epoch 455/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1035 - accuracy: 0.9669 - val_loss: 0.7696 - val_accuracy: 0.7686\n",
      "Epoch 456/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1201 - accuracy: 0.9570 - val_loss: 0.8001 - val_accuracy: 0.7486\n",
      "Epoch 457/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1259 - accuracy: 0.9612 - val_loss: 0.7109 - val_accuracy: 0.7743\n",
      "Epoch 458/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1221 - accuracy: 0.9570 - val_loss: 0.6554 - val_accuracy: 0.7971\n",
      "Epoch 459/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1315 - accuracy: 0.9612 - val_loss: 0.6863 - val_accuracy: 0.7743\n",
      "Epoch 460/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1360 - accuracy: 0.9537 - val_loss: 0.8048 - val_accuracy: 0.7600\n",
      "Epoch 461/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.0991 - accuracy: 0.9711 - val_loss: 0.7354 - val_accuracy: 0.7800\n",
      "Epoch 462/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 0.1205 - accuracy: 0.9620 - val_loss: 0.7721 - val_accuracy: 0.7657\n",
      "Epoch 463/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1233 - accuracy: 0.9579 - val_loss: 0.8208 - val_accuracy: 0.7600\n",
      "Epoch 464/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.1045 - accuracy: 0.9669 - val_loss: 0.8603 - val_accuracy: 0.7600\n",
      "Epoch 465/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1176 - accuracy: 0.9661 - val_loss: 0.7257 - val_accuracy: 0.7629\n",
      "Epoch 466/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1146 - accuracy: 0.9628 - val_loss: 0.7662 - val_accuracy: 0.7686\n",
      "Epoch 467/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1282 - accuracy: 0.9545 - val_loss: 0.6578 - val_accuracy: 0.8000\n",
      "Epoch 468/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.1414 - accuracy: 0.9537 - val_loss: 0.8564 - val_accuracy: 0.7400\n",
      "Epoch 469/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1167 - accuracy: 0.9595 - val_loss: 0.7479 - val_accuracy: 0.7486\n",
      "Epoch 470/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1113 - accuracy: 0.9661 - val_loss: 0.7333 - val_accuracy: 0.7571\n",
      "Epoch 471/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.0998 - accuracy: 0.9678 - val_loss: 0.8924 - val_accuracy: 0.7514\n",
      "Epoch 472/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1238 - accuracy: 0.9603 - val_loss: 0.7592 - val_accuracy: 0.7514\n",
      "Epoch 473/500\n",
      "1210/1210 [==============================] - 0s 152us/sample - loss: 0.1178 - accuracy: 0.9562 - val_loss: 0.9011 - val_accuracy: 0.7429\n",
      "Epoch 474/500\n",
      "1210/1210 [==============================] - 0s 152us/sample - loss: 0.1069 - accuracy: 0.9686 - val_loss: 0.7984 - val_accuracy: 0.7514\n",
      "Epoch 475/500\n",
      "1210/1210 [==============================] - 0s 151us/sample - loss: 0.1090 - accuracy: 0.9595 - val_loss: 0.8806 - val_accuracy: 0.7400\n",
      "Epoch 476/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 0.1033 - accuracy: 0.9711 - val_loss: 0.8260 - val_accuracy: 0.7571\n",
      "Epoch 477/500\n",
      "1210/1210 [==============================] - 0s 149us/sample - loss: 0.1121 - accuracy: 0.9653 - val_loss: 0.8633 - val_accuracy: 0.7429\n",
      "Epoch 478/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.0978 - accuracy: 0.9653 - val_loss: 0.9266 - val_accuracy: 0.7514\n",
      "Epoch 479/500\n",
      "1210/1210 [==============================] - 0s 150us/sample - loss: 0.0959 - accuracy: 0.9678 - val_loss: 0.8609 - val_accuracy: 0.7571\n",
      "Epoch 480/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1282 - accuracy: 0.9545 - val_loss: 0.7979 - val_accuracy: 0.7543\n",
      "Epoch 481/500\n",
      "1210/1210 [==============================] - 0s 150us/sample - loss: 0.1256 - accuracy: 0.9620 - val_loss: 0.9354 - val_accuracy: 0.7543\n",
      "Epoch 482/500\n",
      "1210/1210 [==============================] - 0s 152us/sample - loss: 0.1201 - accuracy: 0.9628 - val_loss: 0.6973 - val_accuracy: 0.7571\n",
      "Epoch 483/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1303 - accuracy: 0.9603 - val_loss: 0.7261 - val_accuracy: 0.7657\n",
      "Epoch 484/500\n",
      "1210/1210 [==============================] - 0s 151us/sample - loss: 0.1144 - accuracy: 0.9603 - val_loss: 0.7921 - val_accuracy: 0.7543\n",
      "Epoch 485/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.1121 - accuracy: 0.9612 - val_loss: 0.8178 - val_accuracy: 0.7486\n",
      "Epoch 486/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1139 - accuracy: 0.9603 - val_loss: 0.8075 - val_accuracy: 0.7514\n",
      "Epoch 487/500\n",
      "1210/1210 [==============================] - 0s 151us/sample - loss: 0.1537 - accuracy: 0.9430 - val_loss: 0.7997 - val_accuracy: 0.7514\n",
      "Epoch 488/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1201 - accuracy: 0.9620 - val_loss: 0.7182 - val_accuracy: 0.7657\n",
      "Epoch 489/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1216 - accuracy: 0.9579 - val_loss: 0.8545 - val_accuracy: 0.7543\n",
      "Epoch 490/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1268 - accuracy: 0.9562 - val_loss: 0.7567 - val_accuracy: 0.7514\n",
      "Epoch 491/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.0964 - accuracy: 0.9694 - val_loss: 0.7830 - val_accuracy: 0.7571\n",
      "Epoch 492/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1328 - accuracy: 0.9562 - val_loss: 0.7353 - val_accuracy: 0.7629\n",
      "Epoch 493/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.1325 - accuracy: 0.9496 - val_loss: 0.7240 - val_accuracy: 0.7571\n",
      "Epoch 494/500\n",
      "1210/1210 [==============================] - 0s 148us/sample - loss: 0.1315 - accuracy: 0.9545 - val_loss: 0.8541 - val_accuracy: 0.7400\n",
      "Epoch 495/500\n",
      "1210/1210 [==============================] - 0s 147us/sample - loss: 0.1197 - accuracy: 0.9645 - val_loss: 0.6948 - val_accuracy: 0.7600\n",
      "Epoch 496/500\n",
      "1210/1210 [==============================] - 0s 145us/sample - loss: 0.1081 - accuracy: 0.9669 - val_loss: 0.8817 - val_accuracy: 0.7400\n",
      "Epoch 497/500\n",
      "1210/1210 [==============================] - 0s 146us/sample - loss: 0.1135 - accuracy: 0.9603 - val_loss: 0.7879 - val_accuracy: 0.7629\n",
      "Epoch 498/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1220 - accuracy: 0.9595 - val_loss: 0.8622 - val_accuracy: 0.7629\n",
      "Epoch 499/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1209 - accuracy: 0.9554 - val_loss: 0.8030 - val_accuracy: 0.7486\n",
      "Epoch 500/500\n",
      "1210/1210 [==============================] - 0s 144us/sample - loss: 0.1078 - accuracy: 0.9636 - val_loss: 0.7932 - val_accuracy: 0.7657\n"
     ]
    }
   ],
   "source": [
    "##Training\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 500\n",
    "input_shape = (sequence_length,90)\n",
    "model = createLstmModel(input_shape, num_classes)\n",
    "\n",
    "history = model.fit(x_train_2, y_train_2,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test_2, y_test_2),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x208a23fb948>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydd3wcxfXAv0+9F1uSu9x77xVsY4rpnQCGhF4Dof4SEggEQkISCKE4oQUIobfQTDG4YOPee5dl2ZKLrC6r383vj9m92ysqtiUXab6fz33udnd2dmZv982bN2/miVIKg8FgMDRfQo53AQwGg8HQtBhBbzAYDM0cI+gNBoOhmWMEvcFgMDRzjKA3GAyGZo4R9AaDwdDMMYL+BEFEponIzCbK+00R+WNT5H2iICKZInJ6E+V9iohscWz3FpFVIlIiIneLyEsi8kgTXPe3IvJaY+d7tIhIuoiUikhoHWmUiPQ4luVyXPuEvG/HEyPojyEiMkFEFopIkYjki8gCERkJoJR6Ryl15vEuoz+1vbAiEiEiz4jIHuul3ykiz1rHSh0ft4iUO7anichjVr53++V5j7X/sWNUvQahlJqvlOrt2PV/wFylVLxS6nml1G1KqSeO5hoiMklE9vhd909KqZuOJt+mQCmVpZSKU0q5AERkrogcl3KeTPfteGIE/TFCRBKAr4AXgFZAB+APQOXxLNdR8BAwAhgFxAOTgVUAlhCIU0rFAVnA+Y5971jnbwV+4Zfnz639JzqdgQ3HuxDHAxEJO95lMBw+RtAfO3oBKKXeU0q5lFLlSqmZSqm1ACJynYj8ZCe2NNs7RGSbZSJ4QkS6i8giESkWkQ9FJMJKO8nSrH8rIgctM8a02goiIueJyGoRKbR6GIOOoD4jgf8ppXKUJlMp9dZhnL8MiBGR/laZ+gPR1v5aEZGbRWSTdU82isiwIGlGWfepUET2isiLjnslIvKsiBywelZrRWSAdewcK88SEckWkQes/R6tUURmoxu1F60eSi9/05iIXGjd32IR2SEiU6391zvKniEit1r7Y4FvgPaOnk97q+fztiPfC0Rkg1WvuSLS13EsU0QesOpTJCIfiEhUQ/4IETlXtCmqWER2O3tUItLFehZvFJEsYLZjX5iIPAmc4rgfLzqyPt16fgtEZLqIiJXndaJ7s89adckQkXHW/t3Wf/MLRxkiReRpEckSkf2iTWXRh3Hf7J50oZX/dQ25L80JI+iPHVsBl4j8R0TOFpHkBpwzFRgOjEGbC14BpgGdgAHAVY60bYEUdE/hF8ArItIbPyzB+DpwK9AaeBn4QkQiD7M+i4H7RDdGA+2X+DD5L1qLxypznQ2FiFwOPGadkwBcAOQFSeoC7kXfj7HAFOAO69iZwKnohjcJ+Jkjj38Dtyql4tH3d7Z/xkqp04D5wC+tHopPD0RERln1eNDK/1Qg0zp8ADjPKvv1wLMiMkwpdQg4G8hx9Hxy/PLtBbwH3AOkAl8DX9oNmMUV6GemKzAIuC7IvQnGIfQ9TQLOBW4XkYv80kwE+gJn+d2P3/ndj186Dp+HVggGW2VznjsaWIt+Bt8F3rfS9gCuQTcccVbav6D/ryHW8Q7A7xt439LRjcEL6Ps2BFjdwPvSbDCC/hihlCoGJgAKeBXIFZEvRKRNHaf9RSlVrJTaAKwHZiqlMpRSReiHd6hf+keUUpVKqR+BGeiXy5+bgZeVUkusnsV/0OajMYdZpT+jX8BpwHIg26mFNZC3gatEJBy40tqui5uAvyqlllm9iO1KqV3+iZRSK5RSi5VSNUqpTHRjNtE6XI02NfUBRCm1SSm113Gsn4gkKKUKlFIrD7M+ADcCryulvldKuZVS2UqpzVa5Ziildlhl/xGYidaGG8LPgBlWvtXA0+ge0DhHmuetHlY+8CVaqNWLUmquUmqdVd616AZlol+yx5RSh5RS5Q0sL8BTSqlCpVQWMMevPDuVUm9Ydv4P0MrL49bzOxOoAnpYCsTNwL1KqXylVAnwJ/Tz0hCmAT9YPelqpVSeUsoIekPTYQmV65RSHdEaY3vgH3Wcst/xuzzIdpxju8DScGx2Wfn70xm43+rGFopIIfolC5a2VqxGYrpSajxaE3wSeN1pTmhAHlnAdvSLu00ptbueUzoBO+rL1zKnfCUi+0Sk2Mo/xbrmbOBFYDqwX0ReET1+AnApcA6wS0R+FJGxDa1LQ8po9eQWix6IL7SuldLAfNuj/1OseriB3Wjt1maf43cZvs9HrYjIaBGZIyK5IlIE3BakXPX9N8Goqzz+zzJKqWDPdyoQA6xwPK/fWvsbQoOemeaOEfTHCUvLexMt8BuDZMtmaZMO5ARJtxt4UimV5PjEKKXeO9ILW+MN04ECoN9hnv4WcD/1mG0sdgPdG5DuX8BmoKdSKgH4LeAxLVmeMsOB/miTwIPW/mVKqQuBNOAz4MPDqEedZbRMY5+gNfE2SqkktPnFLld9y8jmoBtpOz9BC7HsIyijP+8CXwCdlFKJwEuOctnUVb6mXAL3IFro93c8r4nWQH9Drt3QZ6ZZYwT9MUJE+ojI/SLS0druhLaxL27Ey/xBtNvjKWj76EdB0rwK3GZpcSIisdZgXHwd+UaISJTjEyraFXKSNSgWZplt4rE8bw6DD9B284YI1deAB0RkuFX2HiLSOUi6eKAYKBWRPsDt9gERGWnVPRxtm65Aj51EiHb9TLRMI8VoW//h8m/gehGZIiIhItLBKkMEEAnkAjUicrZVb5v9QGsRSawl3w+Bc618w9GNYyWwsCGFEj14OqmWw/FAvlKqwhpjuLoheTrYD3Q7zHMahNVzeRU9npEGYN1T295f3317Bz0ofIX1nLYWkQaZtJoTRtAfO0rQA1BLROQQWsCvR7+wjcE+tEadg364b7Ntw06UUsvRNs8XrfTbqX/QbgNaq7I/11vfz1jXPQjcCVyqlMo4nEJbvYEfGmL7VUp9hDYRvYu+n5+hXVX9eQAtrErQQuIDx7EEa18B2hSSh9ayAa4FMi1zz23oQcHDQim1FGugFSgCfgQ6W7blu9ECu8Aq3xeO8zajbeMZlomivV++W6zyvIC+3+ej3Var6iuTpVyUAutqSXIH8LiIlAC/5/B7Ms8Bl4n2rnn+MM9tCL9GP6eLrf/mB6A3NOi+ZaFNZPcD+eiB2MFNUMYTGlEm8MhJj6WpvW3Z/g0GH0TkGrTp46HjXRbD8cFMfjAYmjlKqfq8mQzNHGO6MRgMhmaOMd0YDAZDM8do9AaDwdDMOeFs9CkpKapLly5HfP6hQ4eIjY2tP2EzwtS5ZWDq3DI40jqvWLHioFIq6ESyE07Qd+nSheXLlx/x+XPnzmXSpEmNV6CTAFPnloGpc8vgSOssIgHLgdgY043BYDA0c4ygNxgMhmaOEfQGg8HQzDGC3mAwGJo5RtAbDAZDM8cIeoPBYGjmGEFvMBgMzRwj6A0GQ4umqKyaiurA0APLMvNZvbvwOJSo8TGC3mA4ydlXVMGBkorjXYwmY0lGHl+tDRYsLThZeWX8+6eddabZur+Eqf+Yx4LtBxn8+EymvbbEc6zG5eblH3dw+UuLuGj6As/+b9fv5ZJ/LqCu9cE25BRxwYs/kX+o7jAB67OL6synsTnhZsYaDIbDY8yfZwGQ+dS5QY+XVFTz5282839n9SYpJuJYFs2HBz5aw668Q3x02zif/VU1bvYXV/Dq/AwePrcfEWG++ufPXtFB2A6WVNImIYopfdsEpHFy8T8XkHeoCgFE4NoxnRERQkO80RH/8cNWNu8r8Qj4FbsKPMemz9nBsz9s9WzvKSijY3IMd723imqXYlFGHiEijO7aChFhxa4CZm7YR0RYCC/M3g7A7/63jm6psTx4Vp+A8s3bmsvPX1/KU5cM5IoRnZi1+QCn9EwhKjy0gXfy8DGC3mBoJiil0KFkoayqhv+tyubKkel8tjqHd5dkERUWyu/Prz+kr8utuO/D1Vw7pjMjugQL4KWpqnHzyco99EyL440FmdwxuTttEqJIiYv0pPn3Tzupcbm5dWJ3Pl6xx5O/LXQf/3Ijry/YSUxEKGVVLkora3jiwgHERmrRVO32ar2PfbkRgISoMNY8eqanrk7eXryLPEubfvwrnf6F2dvpmhLLJ7d7G5iqmkBt+o53VhAWEsIXa3x7D4sz8rlseAwpcZHsLarg6ld14/D78/px1ah0Lv1XYDTHb9bruOh3Tu5BTISvmN1+oBSAddlFFJXrRviR8/oxvkdrYiOaRiQbQW8wHGfyD1VRWlFDeusYAP67KJPR3VrTq01dYXw1RWXVnt+788s9edzxzkrmbsmla0osNS43ALvyDgXNY+aGfbw8L4OXrhlOanwkmXmH+Hx1DiEidQr66XO289ysbZ7tGev20j01lgfP6k1JRQ2frsxmUUYeABN7e9fa2rS3mJkb9nHLxO68vkCbWMqqtI3805XZ/Lgll6cvH8zfv99Kl6hq/CmuqGHu1lz6tUugTUKUZ7/brXh65paA9PmHqsg/VIXbrQgJEdZnF/HDpv0B6b5et8/z+9Reqbxy7XAGPTaTb9btZUinpAA7/otztpPeKqbW+wNw5rPzGJaezJ2Te5AQHcZXa/ayZX8JoKOaf7pSx3ZfuauAOZsPUFBWxYOD6szyiDCC3mA4DP723WZO65PG8M51a7phIUKIpbVWVLuICg/l89XZVLsUlw33jfg49R/zOFBSyc4/n8OhKhePfL6B8FBh25PneNIopahxK1zWJyxEmLlxH60d2vP6nCLSW8fw7pIs5m7JBSCvtIq9Rdp+vza7iAPFFcRHhaNQfLE6hyHpSdzy3xUAzNl8gCtGdmKbJYjmbzuIUorVuwv5cPluHr9wALM2HeDBj9Yw+4FJbM8tDah7xsFD3Pb2yoD9U/8x3/P795+vZ2VWIVn5ZUHvX96hKq5/cxmgg9y2T4wiNSGKNY6B0evf0MdT4iKpqnHxt8sH8+aCTArLqvntOX3409c6XHJcZBillTUAbNxbzIAOiUyfs93neucPbs+Xa3IY3bUVh6pqWJ9dTJv4SKLCQ0mJi2DW5gPM2nzA55yh6UmsyirktZ8yiAgLISEqjIOlXrv8J7eP49J/LWRPQTl7CsoDegkAewvLyTio7+GMdXsBuGpUJ3Ro28alQYJeRKaiAwCHAq8ppZ7yO94ZeB1IRZfyGqXUHuuYC29Q4iyl1AWNVHaD4ahwuxXvLcvikqEdiY6o3T765ZocRndrRUJUONPn7GD6nB212sOVUlz+8iIGdUjkiYsG8Pfvt/L8rG28ef1IfvX+agBO7ZXCN+v2sSwzn6HpyRwoqQRgR+4hyqq0UKp2Kd5YsJNTeqbSIy2OZ2Zu5cU5ZZy+ezlLMvJ5/KL+3PvBGp9rb8gp4pyB7fh05R6SYsIpLKtm7Z5CXp2vtea80kpufXsFq7IKOXdgO2as20ubBG9D8ffvt/L9pv10SIoG4GBpJe8v281Dn+rXNzIslDcXZgJaKy8u92rbQ9OTuGJEJ0/auliZpQX2Z6sbNsD6t8sHM657a4oraqh2uZn8t7mUWML7YKm+d7dajRXA5cM7eQT9xN6pzFirhegHy3bz4Mdr2bS3mKtGpfPo+f1wuRWxkWE8en4/UuIi+dt3m1mfXUyrOD2WUeMOPmB6ybCOrMoqZHFGPuN7tCYlLpLPrfq8c9NoOrWK9qR9+vLBLNx+kE9XZfvk8dP2g1S7FPef0YtnvtdjAgM7JEF54wv6er1uRCQUmA6cDfQDrhIRf0Pf08BbSqlBwOPAnx3HypVSQ6yPEfKGw2ZfUQU7ckvZfqCkQemVUry3NIsNOUVsyCmipCKw+w8wb1suv/vfev78zSZAe2I8+vl6xj81mzmbD1DtcjNvay53vbeKG95c5uNJkV1YHpDfwdJKZm06wJrdhZ7BvU8su/RHy/d40o16chaPfrGBr9bu5QnLjgzw2BcbePnHDM/2H77cyFWvLuZASQUvWlroD5sOUFJZ4yPk4yPD6JkWx/rsYvJKK1mZVcC1YzoTGRbiEfJR4SG0T4pmlSVkbQ1yf7EWlPFRYewrruD7jft5c2EmyTHhAD6C2xbyAHuLyslzaLDtEqNon+QVbhGhIXx461jevnE0CVFanzx3UDsfO7lNZ8vc9Ob1I332331aDx4aFcX4HimICInR4aTERfL8VUPp3DqGf00bxht+5wAkx2oh3SYhksuHd6R7aizhocJ/F+9i095iWsVGcOmwDkSFh3rGAuxxBdue7rYE/L+uGca00em0svKc1DuVt24YxbRR6Z7rndIzlbun9KRVbARf330K43ukkOroaV02vCO/OSdwULbapa8xuU8a95zeE4DBnRID0jUGDdHoRwHblVIZACLyPnAhsNGRph9wr/V7DvBZYxbScOKzLDOfy19axLwHJ3vsxPWhlGLMn2cxbXRn7p6iH3TbzOHE9ioB2PrHsz0eFz9s3E9WfhnTxqSTW1Lp6Tpf+coiKqrdnnNCBLY9eY5nADCnUAspW5O2hd+Zz87znGObDmy0EPUKts17i+mQFM2WfSVU1bgZ2DGR81/4yWMmyThYysLtBz0Ngi1Yndw0oStb9pcwfdowpjzzIz9tP+g59uj5/XhvaRZb95cy6slZAefaPHHRAMZ1b81Lc3fw7fp9PD1zK26lzRGfrNhDjlWe357Tl6/W7mVPQWADBXDeoHa8t3S3t2yndGPG2r1s3FvMA2f24umZW33S//oTX829TUKUpyfQp208395zqufYp3eMY3lmAVdawvGKER1JbxXD9Dk7GJqexMCOiXyzbh8Te6XSqVU0u/N1Gc/s35aD2wLv2+Q+aUzuk+bZ7pYaS0buIab0SeOe03sBsPihKURHhJIYHc6k3mlc8fIilu7Mp2daHN/fN7HW+9kuUdv8U+O1oB7euRXDO7fiN2f34YY3l/HAmb0Z0EEL479fMZj/LNrFuQPb0alVDCsfOcOTj4jw3JVD6Jmmx1lax3oF/8x7T2VXXhk3v6XjbvRuG0//9gmcPaAdvdvGM9f3VjcK9caMFZHLgKlKqZus7WuB0UqpXzrSvAssUUo9JyKXAJ8AKUqpPBGpAVYDNcBTSqmARkBEbgFuAWjTps3w999//4grVFpaSlxc3BGffzJyItT53+sqmZ9dw7Q+EZzRJdznWKVLUeWC+AhfL4mMIhePL9KC6M2psazcX8Pzqyp5dGwUSZFCbLhQ4YK7Z3ttuU+Mj6ZTfAj7Ckr5zRKdX0QoWGN5dE4IYVexG3+mpIcRGSpc0TuCW78/RKVjXC0mDJ46NcbnOsG4b3gkf1+hG4czO4dRVgM/ZWsTwvndw/lyR/CeQ6f4EHaX+Jbpuv4RTOrkvU8P/lhGbrkiLUYY1TaMy3pFMDurmrc2Bvpj3zU0khdWVXruG+h7+cSiChQwoHUoD4yM4rpv9eDrAyMiGZASxhOLytlR5ObcruHMyqqmY3wI2wt1ue4YEsk/V1fSNkY4p1s4Y9uHUeWC4kpFUpRw+w/63oxpF8rivYGTi37WO4LT0sO444cyzugczpV96nfjLKtWRIeBS0G1G6LDhLxyNzmlbjonhpIQIQ16tjOLXBRUKoakhgb1xAH4ZGsVX2ZUMzQtlF8NiwqaBsCtFItyahjdLoywkOB5HSn2//HCaTGECNw5q4xuiSH8fmy0T7ojfZ8nT568Qik1Itixhmj0wWrr3zo8ALwoItcB84BstGAHSFdK5YhIN2C2iKxTSu3wyUypV4BXAEaMGKGOJqKMiUhzdOzILSUtPpL4qPB60zrd+X4oXMf87CzcCW1p37eLj8fIDW8uY/bmA2x/8mxWZhUSGxlK//aJrP5hK7CNTq2imTRpEr/5k9Zcp69THCytYGy31h6vDZvYDr0YP7g9PX/3jWdflUPuBBPyALOy9OP40OUTqPx2js+xshrqFfIA7bv1gRXaZDJzl85PBJTCR8jfOKGrz4Sdi0Z28/hXP3/VUKb2bxvgB/7Xtgf4/RfrmXH3KSRY975kTQ5vbVwVUI77rpjC6tKl9G2XwKRJfQGYBAwbVkh2QTlT+qYRFR7K3dVbWZKRx52XjkFEeHvXMnYUHeCO80fzQtsElmbmc+Uri0mLj+T8iaP45+r59OmUwu+vGRVY+R9mAPDe3WfR9aGvAfjylxO47KWFVNa4+dXFE2ifFM1HPQvomRbXoOenITTWsz12gouuc3YwtX9b+rVPqDPtaUd9tVr4Vt/Dc0+fREiI0K53Ad1T4kiM8b1XTSHDGiLo9wCdHNsdAZ9RFKVUDnAJgIjEAZcqpYocx1BKZYjIXGAo4CPoDcefapcbAaY88yND05P43x3jASiuqGbQYzP5x8+GcNHQDp70VTVuLnjxJ0Z2acXjF/Yn2zIJvLc0i/eWZrH5iakszshjaHoysy2PhR4O4QxwmtX93p1fzmlPz2Vfsdbu7QE2p5C3hf6Xa3KIDDvyiSXvLs3y2R7YIZHswnJax0bQPTWObzd4Xez+fsVg7vvQawt/4CP9+5ox6by9WOfz8jXDuf+jNZRU1HDuoHY8e8UQ3Er5CPpTeqbywuztRIWHcMHg9kHLNblPGvP7+IqYxOhAYfnZneMREf574+iAY0M6JTGkU5Jn+74zevkc/9MlA5mwdi/92iUgIgzskMjorq14+Nx+9Gkbz6Pn9+PcQe2Cls9GRHj/ljGUV7sY2DGRd28eTW5Jpcc+Pyw9uc7zjxeRYaEB9+N4YXtjHct71RBBvwzoKSJd0Zr6lcDVzgQikgLkK6XcwENoDxxEJBkoU0pVWmnGA39txPIbjoKCQ1VEhIUQGxnGqCd/INqyjds2a4Bt+7X712s/ZXDR0A7kH6pi9uYDtI6NYPO+EjbvK2FCzxQy83w14vnbDnpskLUx2+GylnFQd2tDBPwdHWIjQnnh6qF8vGIPT32zmY17iz3HRnROZnS3Vkyfo3WH+Kgw2iZEse2A1/Xv3ZtHEx4awivzMvhw2W6fvD+6bSzVLjexEWGUVNb4CPpLhnX0CPr4qDBKKrQW/8SFAzyCvlebeF77+Qg+W53NkxcN9LzETnq1ieON60bSq239fvFOkvw0vR5JIT6C/HBJi4/iuvFdPduxkWF8cOtYz/b1jmP+9GoT53k+xnRr7dlfl5upwZd/ThtGZi1zGZqaegW9UqpGRH4JfId2r3xdKbVBRB4HliulvkD3HP8sIgpturnTOr0v8LKIuNEePk8ppTYGXMRw1ChLizyjXxs6t647gvz8bblkHjzEI59voG+7BL751SkUlFVTQKCNeZ81mGebE+75YDXztuYSExFKXGQYidHhHte26PBQyq1JJQ99utYnn6tGpfPkRQNwK8VlLy0KuljU4E5JJESFMX+bd1DykqEd+PvPhgBw8yndeGthpmeAEWDqgLZMG92Z8io3E3q2Znh6K9xKkV1Yznkv/ATAuO4pgJ5C//1G70SZv142iKjwUM/gr+0dAjD7fj1gd+OErqzbU8STFw/gD19uJDREEBHG92jNgu15dGoVQ5eUWEY7hB/AuzeNZkduKddY0++dg4cNxV+j9x/jOJbMvLf2AUxDwzhnYN29paakQX70Sqmvga/99v3e8ftj4OMg5y0EBh5lGQ0NILe0kj/O2MRHy/fw3b2n1pn2kc/WezTwTXuLPa5kTr5dv4+x3VuzK19rIPFRYVRUu9hkadNlVS5euGoorWIj+PnrS7l6VDq92sbzyGfrAThYWsWYbq1YnKF9gtsmRBESIoQgfHbneKa9tpgF2/MY060VeaVVbDtQymXDO7Inv8xH0Kc6/LxDQ4SLh3XwaO8ASTERREcETu1Pjo1g1SNn4HI4G0zomeL5/dyVQ7hwSAefc0SE6VcPo0daHN1S9WDYI+d58337Jq+55NWfjyD/UJXP+ilOxvVIYVyPlKDHGopT0D9+YX+SSupeqMtgqA0zM/YEYPXuQmpc7jqnm9fH/iJt1y6uqCa7sNzj6hYMf7F+9WuLA9Lc9rbW0qPC9aDhdxv2M+DR73wmkJzWJ43YyDBWPnwGiTHhzN7sO638nZvG8PTMLfxr7g4fgQswPD2ZBdvzGNIpmd+c3YeCQ1UkxYSzZk8RL8/z+pI7/ZEBzh7QjulzdnBW5zDOGTuA8wcFt3mD15/aJj4qnOvGdeHtxbvo1y74gFx9NmqbmIiwgDVMGhvngObPx3Zh7tzMJr2eoflilik+Abho+gIue2nRUeWRU6QHQ/cWVTD+qdl8uHx3QJoPlmVx+UsLySksp2OytyGwte5gOP3R/WcJ2pNNbK8B58qId0zqTmiIMKqrbrx6tfF1FxvWWQ9E2f7KybERiAiDOyZyz+k9uXSY7zIBNgM6JPLxbWO5tFcEFw7pENQmXhePXdCf7X86h54NWEfmeGP3FgZ3bJpJNIaWg9HoTzI+XrEHl9vN+YPbExEawqNfbKBfuJvqcN+JMP/38VrW7inkiQsHICL85pO1vO8YiLxtYncqql38ccamWq91Ss8Uzh7Qjq/W5rBwRx4PntWboelJzNp0wGcWpE2/dgmM7tqKh87p6xk0nNw7jVn3T6Rbiu+4wcgurTilZwpj/WzbIsI9p/eiqsZNn7bxXOWYgWgzoksr5mYeP3v1sWTRQ6cF9b4xGA4HI+hPEvYUlNEhKdrj4vfrT9bxz2nDeGdJFunxIZw9NDDwxNuLs7hyZDo928T5CHmAjsnRHi+S2njrhlGICG9YKwwO6ZTEuO4pnsFNf6LCQ328OGy6pwZO/oiNDAvqImgTERbCzad2q7N8LYF2ibWb4AyGhmIE/TFiY04x3VJjjyi4wJrdhVw4fQF/udR3XPvOd/UqgfkV7qBrrwD8uDWXPQV64PXe03t5Aip0T42jyFqU6qVrhpESF0m31Dh+2n6Qu9/Tk3TsyVDxljdKz7SWNePYYGguGEF/DCiuqOac5+dz3qB2vHj1sFrTVbvchIfqYZPKGhehIoSFhnh8b2dt8l0q1R7fLK329X0HmPPAJG54cxl/+06vz90jLY7LR3T0CPqOydF0ahXDrPsn+mjcFwxuz7b9JZ5yALx49TB+2n6QtITap44bDIYTFzMY2wQcKK6g3DEv3w4OEWzQ82vHYlelFTXstCYO9X74WyqbdPAAACAASURBVE8km6oaPSDqDHdme8PYWra/Rt+ldQx/udQbweCOSd1p6xDUtrYezKxy/5m9PYuMAbRPiuaKEZ0C0hkMhpMDI+gbmRqXm1F/muUxqwAeE4n/ekuVNS7ueMeb7sGP1zD56bl8Ywn/pZn5XPqvhfy41QoiYS2T+69pw7hypB6kPGdgOzrF679xRGfvlGoR7fFiTwKa2CuVkBBhbLfWniVRDQZDy8CYbhqZDTl6QpFzen+hpdH7+4lk5PpOh/7BMs089uUGzz6nFm+TEh9JmjWRqFebeEa1DWV3iZsJPVNY7pf+0zvGsT672BOJ6L1bxhxBrQwGw8lMgzR6EZkqIltEZLuI/CbI8c4iMktE1orIXBHp6Dj2CxHZZn1+0ZiFPxFZslMvxBXriFhUWK418RAR3G7F24t3sSGniN9/vj7g/Im9Uj3BIPyxJ0F1TYnlpgndeOGqoZwzsC2ndgznwiHtuTyIeaVHWrzPYmQGg6HlUa9G74gwdQZ6JctlIvKF35o1doSp/4jIaegIU9eKSCvgUWAEekLmCuvcQDX1JGZVVgEb9xYzbXRnz9ICVS63J9q9R6MXeGneDv76bWAAY9CNw4tXD2XgYzM9+y4e2oH/WSHIFvzmNJ+lgc+3VkJMjBSeu3Jok9XPYDCc3DR1hKmzgO+VUvnWud8DU4H3jr7ox4fZm/czsksrn+npF/9zIQBXjOhErhW1qNqlWJ9dROfWMV4bPbBoRx6J0eEkRIdRVFZNscOXvVOrGOKjwpk2Op13lujVEROiwrhzcnfPUgC1BVawufmUrqTXs6iZwWBoWTTEdNMBcM622WPtc7IGuNT6fTEQLyKtG3juScP+4gpueHM5n6zYg9utuPHNZSxwhH/LPHiI3JJKT1CJC6cvYNSTs9hvrbNeXu1ix4FSTuuTxuz7J7Hs4dOZYC181TYhipeuGQ7AHy8awC8n9wAgNCSEB8/q47O8bF387tx+XDumc6PV2WAwnPw0dYSphpzrH0qQuXPnNqBYwSktLT2q8+tiS752mVy+YRutSjOZtbmMWY5B1//NWcKeg1X0SQ5hrXaUocrl5rs1Wju3lwKW0gMsmK/jk3YK09r+nQOFzPXLyLTy2rdH2/Vzsnczd66v/7w/TVnnExVT55aBqXPj0KQRpkRkD3qteue5c/0vcLKEEjy4Yg8sXUNc67b0HtwV5szzOV4d3578ip1cMrIza3O9KzDuL/Nt284YNZBJ1trUE5XijoJyOrXyDagd2yWfj7ctYtqUYZzSM7XOcpnwiS0DU+eWQVPUuSGmG0+EKRGJQEeY+sKZQERSRMTOyxNhCh2s5EwRSbaiTZ1p7TvpyC4sZ7O1Fvveogr+8u3mgDSvzt+JWwUPAffKtcM9vwc6ViMUkQAhD3rRr42Pn1WvkDcYDIb6aNIIU0qpfBF5At1YADxuD8yeTKzYVcCl/1ro2bYnMDlJiYv0xDrt3DqGxQ9NodrlpqLaRVJMhE8w6LrWinfS1OudGwyGlkGTRpiyjr2OV8M/Kfl4xZ5605w3qB3XjetCiAgdkqODRh6KiwzzBGY2GAyGY4VRGWvhm3V7+XRVNqO7tmLzvuJ6058zsB1dUup2a1z+8Om1hp4zGAyGpsII+lq43VqDxhlMuja+/OUEH7t7bRzJEsUGg8FwtBhB76Da5aba5a7TNn7Lqd3omhLLkE5J/OnrTczfdpAOySY4hMFgOHExgt7Bz/+9lEUZeWQ+dS7xkWGUVHpnrf5qSk+em7WN8T1SmNhLe8K8dcMoKmvcRlM3GAwnNEbQO1iUoRcky8gtpaSyhqSYcK4b14XuqXGcN6gdk3qnMjTddylgI+QNBsOJjhH0QTjtmR8B+PXUPj7BqZ1C3mAwGE4WjKBH2+b3FATGXD21l5msZDAYTn6MoAce/t96Pli+22ffi1cPbfDEJoPBYDiRMaEEIUDIA/Rtl3AcSmIwGAyNT4sX9NUud9D93eqZ/GQwGAwnC40VSjBdROaIyCornOA51v4uIlIuIqutz0uNXYGjZW9hhc/24I6JPHP5YLNMgcFgaDY0VijBh4EPlVL/EpF+6HVxuljHdiilhjRusRuHrg/N8ERuAhjROZmPbx93HEtkMBgMjU9jhRJUgG3UTsRvvfoTEaUUSsEBK/Tft/ecQo/UuONcKoPBYGh8RKmAgE++CUQuA6YqpW6ytq8FRiulfulI0w6YCSQDscDpSqkVItIF2ABsBYqBh5VS84Ncwxlhavj7779/xBUqLS0lLq5+gV1WrbhjVpln++XTY4gMOznNNQ2tc3PC1LllYOrccCZPnrxCKTUi2LHGCiV4FfCmUuoZERkL/FdEBgB7gXSlVJ6IDAc+E5H+Simf5SCPR4SpjNxSmKUnRiXFhHPW6ZOP+JrHGxOFp2Vg6twyOF4RpuoNJQjcCHwIoJRaBEQBKUqpSqVUnrV/BbAD6HW0hW4Mci2TDUBHsyiZwWBoxjRKKEEgC5gCICJ90YI+V0RSrcFcRKQb0BPI4AQgt9Qr6Du3Mq6UBoOh+dJYoQTvB14VkXvRZp3rlFJKRE4FHheRGsAF3HaihBI86NDoU+Mj60hpMBgMJzeNFUpwIzA+yHmfAJ8cZRmbhKx879o2Y7q1Oo4lMRgMhqalRa5143Yr1mUXMiw9iccvHMCADvVHhzIYDIaTlRa3BMKsTfvp9tuvWZZZwKCOSUbIGwyGZk+LE/Sfr/Y6DPVvbxYuMxgMzZ8WJ+hDHLMCuqUabxuDwdD8aXGCvtrlnevVNaVlzbgzGAwtkxYn6HOKvN42yTHhx7EkBoPBcGxoeYK+0CvozVLEBoOhJdCi3CuratwcKKnkllO7cfeUnse7OAaDwXBMaFEa/f7iCpSC7qmxxEW2qDbOYDC0YJo0wpR17CHrvC0iclZjFv5wsc027U3Qb4PB0IJo0ghT1u8rgf5Ae+AHEemllHI1dkUagj0QawS9wWBoSTREo/dEmFJKVQF2hCkntUWYuhB431queCew3crvuJBjxYdtn2gEvcFgOEreuxrmPnW8S9EgGmKo7gDsdmzvAUb7pXkMmCkid2FFmHKcu9jv3A7+F/CLMMXcuXMbUKzglJaW1nr+7NUVtI4SliwMCHJ1UlNXnZsrps4tgxOhzrGlmfTd9CyrhzxJTbh37s2kLTNgywzmMqZRr9cUdW7qCFMNOfeYRJiqqHZx5+zvuWhoJyZNGnjE+Z+ImCg8LQNT5+PEB9fAoUwmtK+G/o6yzNVfjV2+pqhzQwR9QyNMTQUdYUpEooCUBp57THj4s/UcqnJx3qD2x+PyBoPhpMXSV53xteuJtX2i0aQRpqx0V4pIpIh0RUeYWtpYhW8oFdUuvlqbw1Wj0hnbvfWxvrzBYDgRmPkI7Fp45Oe7qry/q8uOvjzHkHoFvVKqBrAjTG1Ce9dsEJHHReQCK9n9wM0isgZ4DyvClFJqAzqW7EbgW+DO4+Fxs2RnPhXVbs7s3+ZYX9pgMDQ1az+EVe/UncbthoXPwxtnH37+9gz68kLvvsqSw8/nONKkEaasY08CTx5FGY8a23++T9v441kMg6FlsPpdiIiDfhfUn7Yx+PRm/T10Wu1pqg8def62mabcEQX1JBP0LWJmbHmVi3BqSF7wJJQeON7FMRgazrLXIHvFkZ2rFMx+EvJ2NG6Z6uOz2+HDa/XvTV/Bhs+O7fWDUVl6FOcW6+/Mn2DB8777ThJahqCvdnFGyHKilr4As//Y9Bcsy9cvmKu66a91LMjbAQtfPPzzCnbBgudOjoGrbT/A1pnH7no758NG/6GuIMy4H149zXffwhf1va2Pkr0w76/w7zOPrIwNpSDT+3zUVPoe+2AafPSLpru2292wdFVBNPods2Hj5/WfW16gv3ctgO8f0dr80TQcx4EWIegrq12kh+RaW8dA6Hx1j37Bdv7o3bfsNSjac3j5KAVLX4X8jMYt3+Hy6mSY+buGd1c3fq7r+9wg+P73UJjV6EVKyV0IOasaL8N3LoV3L2+8/OrjP+d5td7acCoK+Rm0y/kWDh3U/8Xbl9Z/jTLL1FB2sGkb2zUf6DKVF0D+Tu/+JS97f1cUNe41q8th3tMNfzeqgjy7/70YPvx5/efagt6mKNuYbk5EyqtdDAi1hM2qd+DAJt8EJfvrH8w5HDJ/0t9l1gNy6KDWzN698vDyydsOXz8An94S/LjbBUteIaLSus7qd6G4EbxX7QbGHnyyX9KGajEf/lzX16Zk39GXyY8BG/4Cr0xq9HxPKJzC5Pmh9N76L+9/Urq//vOdAqosP/C4/T/XJ4TXflR3D+LQAW95Zz/h3f/N/3l/z/4j7N9Qf5kbyk//0Nda/bZ3X22Nmasafvxb7XnNf6bua5X73Z+iPUbQn4iUV7voLpYAVC54dYpvgveuhM/vaBz7vVJQlqd/F1kTim173qHc4OfUxtbv9HeNXrqB8kJY/jq4avT20lfhmwfpkP2lfqk/ux3+3hdyVjf8GrlbYPss3317lusG5rM79DVsFr3ovfbhULS7/jR1oRSsfMv7cjWGSWzD/w6vUXS7YfkbgaaJOq/xWf3XWP9p7ceCCeAiS2FpSDmcg4cZc7SpwsmuBfp//iZgnUIvrhr49CZ4bUrtaexG51AubP4qeJqlr8C/xtVfZpucVZC1OPixgl2w4B/690/Pevfb74k/W76BLTO821lLfE02sx6vvSFc/jpU+gv63VBlTDcnHBXVbtpx0Luj+hBUOx4KWxCpBtr7nOxZDvsd67sdclznp3/oB8jWwhoa6GTj51pw77Ye9H3rYO9aWPEGfHUvfPtr3Qis+i8AyQVrYY9jwO6ViXXnf2CT9yWaPgrevsT3uMsSIltmaEFgs+hFbZKpi5qqwH0//sVXMFUU63uTtVjbxnO3+qYvL4T1n8Dq9/R523+AL+7S4x4Q2CC7Xbo3Y1+7cHdg4+WkNBc+ug4+vjHwWG1a4cb/WSa5p2vP10lNpbZNvz5Vu//VVOrGYs37vgL84+th9zLt362UNoPUVFlC5t+B+dqaqSvIffbHKbw+uVGbKpzYDadTAdk5Hw5uc6RxKCl7ahkULrXOt+/NpN9CUnrwtAW79PO9+j09RuF8X5y8Mgler2Wx25kPgwQRXWveC55+23e+26+fGWiyyc+AdR/rhnnV21BVphvGr+7VxxMcK7cU7fG9R/Wx/lOvDNj0Ve11XvtR41oWHLSIRdldlaUkUQKxad5uZu5maD9E/3ZbWmpDXh7QD3xMMrTq5tV0HrNeXqf2Wlmk7ZTp1tJAwR5Om+K9WsvsOEI/hKGRvsdfPgXaDda/l73mI3ATSrZpG7OTkv0Qb80bqKmCrd9A3wt0Y/PPMb5ldrJzvjYZ1UZ9vZJg3ggHt2qtqv9F+iX56l7YNlO/PMXZOs0jByHUCu044z4t6EE3bulWeW0tqtRhCirL1/crc76exDLyJq05VhbDo4W6vrsWQUJ7SO6sz9m7xje/am/UMarLICJI0HhbKJbsrbv+NhXWfSjcpd3/9m+A2FRtyz79D75p/20tDTX0Gi1k8jOgy3hY+EJgvln2hB+lxz5K9kEnv3UCN3+t6/3T3wPPd7t1oxUe6zXHhIR6j396C6T2gp9bGq/z/3ztNPjZ29D3fN887Xdqi+WBndID7lmnG7msRb5pnxvku91+KNwyN7CctVFZApu+gPH3eLX69kN1D+Cre6F1T92QdhkP0cm6vnUNsk95FGb9QStO8/7q3b/4JWjdzbvdZoD3Wd23Vj/TNm43hNTybh/YpBvzAZfCOU/rwelOY+CSV/S70GW8/t74BSx9GeLbQbc6elhHSIsQ9LFl1st51p8gsYOeNJGfoV/wzmO1RggN75a/ZnlBBBOUtqDvMFy7xbkqvRpccbb2YGndHQ5s1oKtdXd9bOHzsPifkNpHb9tadeseXsFrC6iGkDkfBl6mf8/7K8z7G1zzCbQbGjy9rcn+57zAYxLi7e3Y30XZ+gG1GzFXNeyY462PP7Y568t7YPv3+rf94oDuGXUeq387B/R2L9Ef0D2bkn26EbP54VFdV9Ba2IFNXuFUWaL9ud+YquvwfzshZyXstUxbrbrqb+dEmPLC4II+xHpV3C7934WEQkodUcr8G7yi3V6hV9usylWWvXndRxCV4HssfZxDyFs8P1QrKdd8Cj2m6P9g0XR9T2ojYw58fIPvPgn11q10n2WC+Ro6j/M2WDYfXQe/zYEwhyJS6tf4RyXp77G/9BX0Kb18BSTUO6CeWLgB8jppM1P/i72uoh2GeRO17uHNZ+ELWoOPbw/3b9L/9aE6TLL9LtS2/sX/9N2/f53+2LRyCP1tM73Xzduu39WQWlbEtcclCjK9zhg5q+DlU6GiEG6dB3P+BFu/1cfaDam9rEdBizDdxFVZGmBiR4hupX9/fL0WADVVXo3e38bnqvEOrNo4ByTdfpN8ldJdUoBpH0NcG91NcwqSty/RZqN/joYXhnmFt21Xz93sm+ewn8Pkh2uvnIQG3+9sFOwHrDgHspcHT19dFnyAafTtcIfDVmoL+k9v0V3g5W/o7Z+e1V4rG/18pk9/TH/bdly3ZV9v79fgOD2SQmrRP/auhr/30y+9zU7HSqSbZ3h7K6C9TQp2esv93pXafGF7g0iIFlLO/3j7D8Gvba93krVQ/3cvjvA9vH+jb5fc376+d41XSNXXK8jfAd/91ndfcpfAdPZz+/YluhGc++e6hTz4jrn4cyhX3yd3Nbx/Fbx1IRxwmCU7jdHXPLBRmxZ3LdQNgr9Hiy3o+56nlaHxv9Lbg34W/Lol+7Twy8/Q39krPYeGrv6t7hF9cZfuudlafOse3vNbOZQLe85BSY7+b/csC7zeL5fDI3m6bK2767z8be6XveG73WWC/h5+vf5uPwyGWW6jmQu8DaKrWj9PZfnaJGcrFSFhXiXQVamFPGiBbwt5gPi2we/RUdIgjV5EpgLPAaHAa0qpp/yOPwtMtjZjgDSlVJJ1zAXYTWOWUuoYTZfzklBpCZnEjr6aCGjNyyPo/Uw3c/8M85+Gm2Zpkwr4unP5ezXMe1qbSGJa625jXJp+eSocgr4gExY5uuQvnwrXzdDdwYSOUOzngpk+1teUMv5XuhtpzwYMiwo+689+wOw0oF8oZwPg1KaqDvmaMGx6nu6r4VaXa43bfqG+ukc/3HYD5fRomvIoTLhXa5kl+/R5BZnQcZQ2ATzTC079P93jKHK4YNbljqlceqzApmAnnPaI1sh2+w3eZa/0raOtXTq9RN65zPdefXm3NuG1GwLtBunnpbrCq00XZPpeoyhbl+HNc3XDcftCSOsbqNE7/0Nnj6WhxKXVfbxwd3CTW0IH/TzuW6u3t36jv3/+BbxlvYr287l5hu+5e1fD/271bve/SN/jzV/Djlm1T+SKTvLdPv0P+lPbuMn7V+txmmAukOC9jnMw2alhO4V+maOx3bdGj+dIqH5uAH5fEGhmOeMJ3cu59FVdFtCmPiepvb2mwDP+oE1fK9/Ux965FLqcAtd9pbXzn/6uTTDOBr1gV8Pcq+PaQBMsEtMoEaaUUvc60t8FONW1cqVU0/RHGsD2AyXUFGThCgshNL4dAX705YW1a/T2y1GQCZEJ2nbpfJmc9upFL8Li6fqhu3mOfiBi07Qm69ToJSRw0tab5+rvAZfCyv949z+wHeJSvYL24pdh4BW+D3ww7Tck3NeVzdbC5/7Zd8DZ6Z6YtwNqggj6zhN870t5ATxv/Z3dT9Nl+eZB73Gnl4ldtri2Wmux69Z2kB4/+O1eiIjR4w1bv4Mxd+prle6Dyb/TdliU/n/+1IFa50B0Gu3r1mfziTXYGpUI3afABj8Pl8pSXyFvYw9Adx6vG6SPrvOdE2Gz5Vt4z6GlKjd8dD3cuTiwJ+ikrjGQ2oiIq/t4QWZg7+7cZ2DYdfr+7VrgO+je2eEBU5avG6wZ9+ntyITgYy29z9ED4k5bdjCi/AS97YRgjzHZdnGbhsz8veE73at55zJtugt3mEoSOwY/Z+8a/Y7GtNbX3v59cFt676nw651eJbDjSG/P3yY62VuPqET97RxHy5yvGyvb68gW8iFh0G2S7ikufcU3z5gU34YJtEZfSKPTWBGmnFyFXtjshODs5+bTQfIoCkuF0DBtF49wrHlTUegVfi4/G71tmvnkRpg+Ur9My1/3Hnd21W3he9sCr301Lk1rFE7BbNvgg9Gmv/f3zbO1kLf3/yYLBl+pH1Sn/XZwEN/8tgO1TbwgU5fRto/X5VX0xlRfr4zWPeGhbAiP8hUyTpe9DsMhzVFmCO4rXZ7v6/cdm6K/I2K8x3cv0T0du3HtMAzCIvTLFxELQ66uveztBkEfx9jCmY6GtO/5cPdq3Rj4kxfEc8I50LhrAfy1a6CQt81OTiGf2Enbe4t2w7bv9ZhIbTRkQPfqD6GnY0ZrZBzzTvmw9vR52wMH+2NS9DMfHqV7GU5Cw709vdxNeuzC5pa5gaaiu1frweyEdt59pz5IUGxB6E9cKvxmt+7lOekwPHh6m+hkLXzj22ol6m6rl9bbCk2dZK2EHu7oeca3h33r9fMfmwpXvQ+/q2M+hy3kf7cPrvsaYvwEvX/j5TzHZvpI3zGIqU/BvRt1wwb6P7LNTN1PC95LC2+a6HeNFWEKABHpDHQFnA67USKyHKgBnlJKBSx80ZQRpqpdivYheeS4k1hr7R8dEk00upu49ceP6GWlXbdqOXlZisjKfCqjUhmUn4fz71bPDUXwCstNS77H8/rkrOJQTEeWLfCaD7rlV5JenO0z6LhP0miLM9yulzXZ5Vg6DwvXZ1K1LXhXNuZQlice449RZ1ExZBhnrfZ2sfe6W9EO4DmdW2VEayIBV0gEoe6GeRYVV8HKRV57/iT7h8OGvT4vhIJeDzOk7HfEl1omLYeZaseOHeyunkvfqB60Kc7mQOo40nIXkrU/nwzHfzQoeTCtCtawb8NPlO7KpgewIKOU6j3eNJJwCe17xNBzu7YxF0d3JKqmmBC3i58Wr0LSrqProXDSd/+PpQXJnvuTUZFI1tK1pOQWMsC/klYDmN3+bHZ3uhhXaCQ1YbFM3PRlnfcmM7wnXfAdRCxxRZBbnkC3qlJ2LPqSWoakKYvuQEx5di1HvSzbuo+ydrfSt/AQabkL2JKxi+KETrWmL9q6gPLotjgtvKu37qIwdy4A4q7G6XQ7d+5cQsb+h867PqRz1ic6uIbFj2syGFlRRYwj/Y+rt6NCdjG4JpJkIDdlLBtknPe5cOY9v47ejIV93sqhT1ES34Nx+68nvCb48743cThb5gVGhZM2NxCSeg2uVdsJH/cWKQcX03urHlQtCE0hPHMFIe4qKiNTWdOAMvnk7Xb53q8g56fkbvc8UwVJA0guXO9zfGleLGUrNoFSnCphuEKjWdL3DygJQUkYQ1f9Gv9lFpdnFFAq4SdshCmbK4GP/ZYiTldK5YhIN2C2iKxTSvmsstSkEaa+nUF7Oci2kD6cbu/f0h72ajttr20veZIOTE+GytXw4+Naa9iVAA4zvODWNs9TH4Cv7qVvme/S+rGdBvteO2I97LbMBe2GwN7VtB06Fb6d600z7Bcek8bgyRfBWt36j5t0FkTW0l0vztFRAoCJp00JeCjaDZ4C+7wCObIqD9oMJPT857weQzbn/UPb2f1ImPo7JgWJpuPhzD8yYOwvdXfWtcDXbm7Rfdz5dO8xCcaNgqpDpG34FL5ZSHpqIunO+zR2Brw8kbYhhVCyBBLTGX/mRYH1VlNg/Rj45EZCQkKJeGAT1FQwye4hqNOh+I+MSuwIy+4EoNuIM+jWfxLkJMKGPwfmCXSYcDUdnCstzguazEOXsRfBLl/tOj4+nvjB42Dnf+ke5RiIfTBD96TeugAObCSm2yjtRmsz+jZtyvr8Dp/8Rk46W2t8JZ9B7gJ69+7D3hLH85CY7jOukViylcS2XcHRcRoyeqLXhRhg5BZ4pjdEJ3uf05rJ8O8d2oQVEga/WsvExA5QdbE2RVpMPO0M/WN/FyhcR+qgKUw69TQIYtFq0Ps7V38NO/9W3Utdnwb5foL+rpUsWLGO8ZPOpF1ETEAWAexIgK3/hNY9SO40EFZrn/SYbmOPLGKT/Rw8uMP7jDnZUgEbgIQOJI+5Vs9vcTBq6tW6RwUwcjMhETFMcI53bf8T2GPA/S6Cs55kRGLHJokw1RDTzeFEiboSP7ONUirH+s5A/721+Pc1DcmR0E7y2F6V7N3p3y2z+foBPUsOtBdDsIWQ0sfqriTowb0h07ymDX/XQmfXbNrH8Ku1vqabu1bCeY6ZfbGp3t/BXPxsIhMC9zm7lqm9vL87Wrpt24G+piHQXf30MQRw51LtyuZPSLj3d/o4r81ytNWbiLcGsEbcoOvWw/IPj4jR3XbbRtvGT7eOjNemhb2rtUfHhMCGR5dXPOUtSB6sG0LnCyjitdfaHhG2ySKhFjvuqFt9zT4QOIfBH9usMem3jv9PeT0mnJ5Hsa113QdY8xw6O1bzvuK/cOaT0KZf4DVsG7FtbvO3v/d2rKsenazTOb03IHBQNL6tHve529EbCYuEcXfp3ym9tfsx6AHHX63RJqlgxPl5hzywHe7bBPdvDZ6+Nmybud3QOt+H1t2pjkjymvjqo+tEuPF7+MWXvnb7o12CI5iQB68HWWof/X75E+rQo51jbTbO8TXlqn2soRFoiEbviTAFZKOFeYDBVER6A8nAIse+ZKBMKVUpIinoNevrGclpXLrJHiJwcer4Sd6dI24InA7uz/YffG3wbQdp+3Fsqlcg9zwTzn/OoznQ1m8yiFNwx6ZoQeQc5PJvGJwPQl2zaIM1Anev0p4DGXMg1WGPHf4LuPglXZbwKN9zQiN87e/JXbUHSWrvwPzv3aCvO+N+PZnJOfMxKV0L9tgU7dLWqqvvJByb9DHaKyW1b+Ax+16c/5xXSAcjsSPcsYSM9Xuo3ZCB7qmMHqX0egAAE6pJREFUvNFbl7hUuHMZrPvQ137ebVLgAN19G7XnjatKu845te171un63rYA0vp53Tc7jw8Ufk4m3KdtynFp3sHe3mdrYRDh34HHKyQ8gt6vjL3P1hNszvsH9JoKf+9DQEc7mK08LjVwX/+L9YCl83kMDdcN2u0LfJ0JbGzha3u0BMu3LiLifF0aT3sEBl6uvWm+urf28+oiJMQ7eSzaodjVNrfjaAmz7Ol9ztETn26Zqx0cUnrBL2pZCsJJ57Fed2dbCWoi6hX0SqkaEbEjTIUCr9sRpoDlSil7rdWrgPeV8plD3hd4WUTc6N7DU05vnaam2uWmW/V2CId+w051lOp8Pdjpv/yrk5Vv+W6n9LIEfWv9Ut8wUz9UToHs/2fFOSJa2eliD/OFCIadl3NadkwrPeCUt82rlYEWjM4H/a6VWsN58xytuTobjVvm1r7ej61tXPSS1gD9X2z7GrUNxNn49ypsJv5aa70NeeDT+qBC6tHSQkIC80rt5dXwbUHjdM2zcWpw/vfDbuDaWr2S1t3hlh91vfx7gHc5BjhDQrTmbr8eMa29M4EjHYL+zqW+2rvdUCV2hCLHq9V9Mtz4g9ftNyI+0D0xsp7/wlO2UJ1fMKISff/TtoNg05fe3tv9m4O75dbHr9b4ztsICfV9NpI6H36eTuze1ahb4fR65hbUxr0bap+nAnqS2k2zvIPJ7YfCHUu0a6b/hLdgTHlMP/PK3WQTpWwaJcKUtf1YkPMWAkH6NMeGwrJq+skuqkOjCXf63ULdI/1p/eGAn/eI/VJGt7JMCI7x6OQu2sOlLtONTYwVs7b9sMBjh8Pti3wbEtAau92FHPQzWPtBYNe7dXdvlzE03CtkOgzXXX3/7r4/YRGBk50ag4jYJtdqAOh/iXZ77ThCm+ecZq5g2P9XXdh28OgkrQC8fqY2cwXTJEV0b8D539ljMV1PDexNjbtbm9+6jIesub7HOo30/k7sqL1nbG6dV/u0/KPhlPu1omPPYq7Pv782YlNqN4nctiDQj/1w6XeRNuF0OaXha0z5U58pRcTb0Nqk1eFV509oWNO8S0Fo1ksgFJRV0V7yKI/tQHiwh/7W+fph27/Rd62Y3mdr7agwS78w4TF6ES4Ivl7NDd/pAVJ/c4W/Ly5o4XrDTN/p83et9Lpy/mptwwIPB7PrOjn/eS3sgwkbW4tvN9gqz3d1u302J0Sg6yn6t/8aMcGwhVFcG23/rY/00do8FRnEHGPT1m+MIiJW/wf+LpCgn6kuDrv+PeuDz2C2BX36OLjg+bqXZzga/MvTFPjfnyNBRDecBqCZC/q9RRWkSSEqtpag4O0sm3q8wzd46l9gwCXajl+w06tl2gtiBRvIjW8bfOpybRpVup93qlMYJx9ll9UmPEp3LYMRm6LXR7G1kWADsgZNVCJc+5nW2p1237qozTxVFw39D5JqGZmwTUrthzadkDectDRrQb+noIyJUkh4Uj32L2fXbsxt3t9OW/eEe/VgZd/DXMHh+m8CTSwnArU1AoZAarNfn0iMv1s3Av0vqT+tocXRrAX97rwy0igkLLkR3JZCw2HQEYSac041NxiaiuQugTNODQaLZi3oC/L2ESE1kNA0K8IZDAbDyUCzXqa4PN+a13Uimk4MBoPhGNGsBb1nIS0j6A0GQwum2ZpulFKEV+TpKV4N8fW99N9H7hNsMBgMJzDNVtCXVNaQ7C7Qgr4hs1HtsHsGg8HQzGiQ6UZEporIFhHZLiIBkWtF5FkRWW19topIoePYL0Rkm/WpYxGTxuVgSSUpUoQrJKL+afkGg8HQjGnSCFMi0gp4FBiBXnFphXWuY/HfpiHvUBWpUkR1VAqhRzoF2mAwGJoBTR1h6izge6VUviXcvwemHk2BG8rBkkpSKMLdGIuIGQwGw0lMU0eYCnZuhyDnNXqEqUVZ1fxMiiiqTmVpI0drOdHwj6rVEjB1bhmYOjcOTR1hqkHnNkWEqa3zdpC6o4jkzqfQrpGjtZxoNEVEmhMdU+eWgalz49DUEaYO59xGpaKqhlYUE2pcJg0GQwunIYLeE2FKRCLQwvwL/0TBIkyhg5WcKSLJVrSpM619TY6U5xMmbkLM8gcGg6GF06QRppRS+SLyBJ5Q1jyulMpv3CoEJ7zCCgNoBmMNBkMLp0kjTFn7XwdeP8LyHTHh5ZagN6Ybg8HQwmm2a91EVdoavRH0BoOhZdNsBX10ZZ7+YTR6g8HQwmm2gj6hah9lRJvlDwwGQ4un2Qr65Op95IamHXkEeIPBYGgmNGNBf4C8MGO2MRgMhmYr6FNcuRSGm4AjBoPB0DwFfXUFCaqY4gij0RsMBkMzFfRlALjCYo9zQQwGg+H400wFfbn+Dos6vuUwGAyGE4BGiTBlpblCRDaKyAYRedex3+WIPhWwRk6TUFMBgAqPPiaXMxgMhhOZRokwJSI9gYeA8UqpAhFxGsfLlVJDGrncdWOZbsQIeoPBYGi0CFM3A9PtEIFKqQONW8zDQ1mmGyPoDQaDofEiTPUCEJEF6BUuH1NKfWsdixKR5UAN8JRS6jP/CzR2hKmv56zmXOBAflGLiE5jovC0DEydWwYncoSpMKAnMAkdXGS+iAxQShUC6UqpHBHpBswWkXVKqR0+mTVyhKnNeyMAuOr00ST2HHvEeZ0smCg8LQNT55bBiRxhag/wuVKqWim1E9iCFvwopXKs7wxgLjD0KMtcLzWV2kafmGDWuTEYDIbGijD1GTAZQERS0KacDCuyVKRj/3hgI02MsgZjCTfulQaDwdBYEabskIEbARfwoFIqT0TGAS+LiBvdqDzl9NZpKlS1dq8kzAzGGgwGQ6NEmLLCB95nfZxpFgIDj76Yh4fYE6aM143BYDA0z5mxUmMEvcFgMNg0S0Ef4qpAIRAacbyLYjAYDMedZifoXW5FuLuS6pAoE3TEYDAYaIaCvsIFUVThCjUeNwaDwQDNUdDXKKKlCrdZudJgMBiAZinoIZIqVGjk8S6KwWAwnBA0O0H//+3dX4xcZRnH8e9vd9vd7R+gpZQ/C1iIkACiRRqEYEhtTKnE1ES4gJhYEowhwYBGYmw0EuFC8EKMN0qIjV4YREEBeyMVaDQxQKj8F6Et9gpC6R+20G53O7OPF+edOgzTduies7P7zu+TTM7Me8478z6zk2fOnjnnfcbqwSCHPBe9mVmSXaIfr8Fcaj7jxswsyS7RT0wWe/Ty9AdmZsD0VJhaJ2lruq0ra+BHMlGHufKhGzOzhkorTElaDNwBrKCY2nhL6ru3/FAKE+kYfd+Af4w1M4PqK0xdDWyKiD1p3SZgTTlDb29iEuZyiL653qM3M4PqK0y16zvS+gKlVpg6MM4gh9i1dx9be6Qyjavw9AbH3BtmXYWpDvuWWmHq0W2PM1c1Th05m5EeqUzjKjy9wTH3htlYYaqTvqU6NFlcMNU3x8fozcyg4gpT/L8gySJJi4DVqa0y4/VgyBdMmZkdVmmFKQBJd1F8WQDcGRF7qgik4VAd5qoGPuvGzAyouMJUWrcB2DC1YXZuol5ngDp4rhszMyDDK2OpHyqW3qM3MwOc6M3Mspdfop+cKJZO9GZmQI6JvrFH72P0ZmZAlonee/RmZs2yS/SabByj93n0ZmaQZaKvFXe8R29mBmSY6Psnx9MdV5gyM4MME/1J8V5xZ8HS7g7EzGyGKKXClKQbJb0r6YV0+0bTunpTe+scOaWqTwanFTMvwIlnVvlSZmazRikVppIHI+JbbZ5iLCKWT32oxzZeq3OGdnFw4ASGBhdOx0uamc14ZVWYmhHGJuqMaBcHhk/v9lDMzGaMsipMAVwr6SrgDeA7EdHoMyTpOaAG3B0Rj7R2LKvC1O6xSS7Ubt6tL+WlHqpK4yo8vcEx94aZXGHqL8ADETEu6Wbgt8CqtO7siHhL0rnAk5JejojtH3qykipMbdv5ASc+Pcr+Uy7rqao0rsLTGxxzb5ixFaYiYndEpPMauR+4tGndW2n5JrAZuGQK4z2qgxM1TmA/k0OLqnoJM7NZp5QKU5KaD4qvBV5L7YskDab7S4ArgdYfcUszMfYBg6rB8ElVvYSZ2axTVoWpWyWtpTgOvwe4MXW/ALhP0iTFl8rdbc7WKU1tf1G8SsPeozczayirwtR6YH2bfv8ELp7iGDtWP7AXgL55i6frJc3MZrysroyNlOj753uP3sysIa9EP1YcuhmY7z16M7OGrBJ938Finps5C07u8kjMzGaOrBK9UqIfPMGJ3sysIatE3z8+ykT0MzTseW7MzBqySvQDE+8xygIGBvq7PRQzsxkjq0Q/Z3yU95nf7WGYmc0oWSX6ubV97NOCbg/DzGxGySrRD9VG+cCJ3szsQ6ajwtQ6SVvTbV2Zg281XHuf/U70ZmYfUmmFKUmLgTuAFRRTG29JffeWMvoW8yf3caDfx+jNzJpVXWHqamBTROxJyX0TsOb4hnoM9UPMizHG+rxHb2bWrJNE367C1Eib7a6V9JKkhyQ15q/vtO/UHRwtFv1O9GZmzaquMNVJ31JKCfbVx3kobqdvYMSlx3qAY+4NjrkcnST6jipMNT28H7inqe/Klr6bW1+grFKCt/wdrpiHS4/1AMfcGxxzOSqtMEVRrGR1qjS1CFid2koXEeyfqDHY3+6fCDOz3lVphamI2CPpLoovC4A7I2JPBXEwXptkMmCwo1IqZma9o9IKU2ndBmDDFMbYkQMTdQDv0ZuZtchm/3d4Tj8/+erFTL6zrdtDMTObUbKZAmF4bj83XHY2IwuzCcnMrBTOimZmmXOiNzPLnBO9mVnmnOjNzDLnRG9mljknejOzzDnRm5llzonezCxzpZQSbNruOkkhaUV6vEzSWFOJwV+VNXAzM+tMaaUEJS0EbgWeaXmK7RGxvKTxmpnZx1RmKcG7gJ8CB0scn5mZTVEnk5q1Kwf4ueYNJF0CnBURGyXd3tL/HEnPA/uAH0bEP1pfoIwKUw2uSNMbHHNvcMzlmHIpQUl9wL2kOehbvA2cHRG7JV0KPCLpoojY96EnK6nCFLgiTa9wzL3BMZejk0M3xyoluBD4FLBZ0g7gcuAxSSsiYrxRZjAitgDbgfPLGLiZmXVmyqUEI2I0IpZExLKIWAY8DayNiOcknZJ+zEXSucB5wJulR2FmZkdUVinBI7kKuFNSDagDN1dVStDMzNorpZRgS/vKpvsPAw9PYXxmZjZFvjLWzCxzTvRmZplzojczy5wTvZlZ5pzozcwy50RvZpY5J3ozs8w50ZuZZc6J3swsc5VWmEpt61O/1yVdXcagzcysc5VWmJJ0IcUkaBcBZwB/k3R+RNTLC8HMzI6m6gpTXwF+n6Yr/i+wLT2fmZlNk6orTI1QTFvc3Hek9QWaK0wBH0h6vYNxHckSYNcU+s9Gjrk3OObecLwxf+JIK6quMHXUvocbmipMTZWk5yJixbG3zIdj7g2OuTdUEXMnif7jVJgCOI2iwtTaDvqamVnFKq0wlba7XtKgpHMoKkw9W3oUZmZ2RJVWmErb/QH4N1ADbpmGM25KOQQ0yzjm3uCYe0PpMSviI4fMzcwsI74y1swsc070ZmaZyybRdzpNw2wjaYOknZJeaWpbLGmTpK1puSi1S9Iv0nvwkqTPdm/kx0/SWZKekvSapFcl3Zbas41b0pCkZyW9mGL+cWo/R9IzKeYH0wkRpBMcHkwxPyNpWTfHPxWS+iU9L2ljepx1zJJ2SHpZ0guSnkttlX62s0j0TdM0fAm4ELghTb+Qg98Aa1ravg88ERHnAU+kx1DEf166fRP45TSNsWw14LsRcQFwOXBL+nvmHPc4sCoiPgMsB9ZIuhy4B7g3xbwXuCltfxOwNyI+SXEdyz1dGHNZbgNea3rcCzF/ISKWN50vX+1nOyJm/Q24Avhr0+P1wPpuj6vE+JYBrzQ9fh04Pd0/HXg93b8PuKHddrP5BjxKMddST8QNzAP+RXEF+i5gILUf/pxTnAV3Rbo/kLZTt8d+HLGemRLbKmAjxUWWuce8A1jS0lbpZzuLPXraT9PwkakWMnJqRLwNkJZLU3t270P69/wSisnyso47HcJ4AdgJbAK2A+9FRC1t0hzX4ZjT+lHg5OkdcSl+DnwPmEyPTyb/mAN4XNKWNP0LVPzZ7uTK2Nmgo6kWekBW74OkBcDDwLcjYl+68rrtpm3aZl3cUVxjslzSScCfgQvabZaWsz5mSV8GdkbEFkkrG81tNs0m5uTKiHhL0lJgk6T/HGXbUmLOZY++16ZaeEfS6QBpuTO1Z/M+SJpDkeR/FxF/Ss3Zxw0QEe8Bmyl+nzhJUmOHrDmuwzGn9ScCe6Z3pFN2JbBW0g6KWXFXUezh5xwzEfFWWu6k+EK/jIo/27kk+qNO05Chx4B16f46imPYjfavp1/qLwdGG/8OziYqdt1/DbwWET9rWpVt3JJOSXvySBoGvkjxA+VTwHVps9aYG+/FdcCTkQ7izhYRsT4izoxi6pTrKWL4GhnHLGm+itodSJoPrAZeoerPdrd/mCjxB45rgDcojmv+oNvjKTGuB4C3gUMU3+43URyXfALYmpaL07aiOPtoO/AysKLb4z/OmD9P8e/pS8AL6XZNznEDnwaeTzG/AvwotZ9LMT/UNuCPwGBqH0qPt6X153Y7hinGvxLYmHvMKbYX0+3VRq6q+rPtKRDMzDKXy6EbMzM7Aid6M7PMOdGbmWXOid7MLHNO9GZmmXOiNzPLnBO9mVnm/gcMb3O/LbEkYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.set_title(\"Simple LSTM classification, arithmetic\")\n",
    "ax.set_ylim((0.4,1))\n",
    "ax.set_yticks(np.arange(0.4,1,0.05))\n",
    "ax.grid()\n",
    "ax.plot(history.history['accuracy'])\n",
    "ax.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "## Save model for real-time\n",
    "# model.summary()\n",
    "# print(global_mean, global_std)\n",
    "task = \"arith\"\n",
    "model.save('./models/'+'simple_lstm_seq{:d}_{:}.h5'.format(sequence_length, task))\n",
    "normalizer_dict = {'mean':global_mean,'std':global_std}\n",
    "config_dict = {'image_size':image_size,'frame_length':frame_length,'sequence_length':sequence_length,'overlap':overlap}\n",
    "\n",
    "pickle.dump(normalizer_dict, open('./models/'+'simple_lstm_seq{:d}_{:}_normalizer.pickle'.format(sequence_length, task),'wb'))\n",
    "pickle.dump(config_dict,     open('./models/'+'simple_lstm_seq{:d}_{:}_config.pickle'.format(sequence_length, task),'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_size = 32\n",
    "frame_duration = 1\n",
    "overlap = 0.5\n",
    "lstm_length = 5\n",
    "autoencoder = keras.models.load_model('autoEncoderWeights/autoencoder.h5')\n",
    "encoder = autoencoder.layers[1]\n",
    "\n",
    "dataPath = Path(r\"C:\\Users\\asus\\OneDrive - purdue.edu\\RealtimeProject\\Experiments3-Data\\ArithmeticCalibrationProcedure\\edf\")\n",
    "X_arithm_encoded, y_arithm_encoded = createImageDataset(dataPath, imageSize=img_size,frameDuration=frame_duration,overlap=overlap,\n",
    "                                      image_format=True, augment_data=False, labels = [\"EasyAdd\",\"HardMult\"],\n",
    "                                      encoded_format=True, autoencoder=encoder,lstm_format=True, lstm_sequence_length=lstm_length,\n",
    "                                      fileNameFormat=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_arithm_encoded, y_arithm_encoded, test_size=0.20,shuffle=True)\n",
    "print(X_arithm_encoded.shape)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Global frame/time normalization\n",
    "global_mean = x_train.mean(); global_std = x_train.std();\n",
    "x_train_2 = (x_train - global_mean)/global_std;\n",
    "x_test_2 = (x_test - global_mean)/global_std;\n",
    "\n",
    "## convert class vectors to binary class matrices\n",
    "y_train_2 = keras.utils.to_categorical(y_train, 2)\n",
    "y_test_2 = keras.utils.to_categorical(y_test, 2)\n",
    "\n",
    "x_train_2 = x_train_2.astype('float32')\n",
    "x_test_2 = x_test_2.astype('float32')\n",
    "\n",
    "print(y_train.shape)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLstmModel(input_shape,num_classes, lstmLayers=2, lstmOutputSize=4.0,\n",
    "                  isBidirectional=1.0, inputLayerNeurons=64, inputLayerDropout=0.25):\n",
    "\n",
    "    dropoutRate = 0.45\n",
    "    \n",
    "    lstmLayers = int(lstmLayers)\n",
    "    lstmOutputSize =int(lstmOutputSize)\n",
    "    isBidirectional =int(isBidirectional)\n",
    "\n",
    "    # Input layer\n",
    "    networkInput = Input(shape=input_shape)\n",
    "    dropout1 = Dropout(rate=inputLayerDropout)(networkInput)\n",
    "\n",
    "    # First Hidden layer\n",
    "    hidden1 = Dense(inputLayerNeurons, activation='relu')(dropout1)\n",
    "    dropout2 = Dropout(rate=dropoutRate)(hidden1)\n",
    "    batchNorm1 = BatchNormalization()(dropout2)\n",
    "\n",
    "    out = batchNorm1\n",
    "    for i in range(1, lstmLayers+1):\n",
    "        retSeq = False if i == lstmLayers else True\n",
    "        lstmLayer = LSTM(lstmOutputSize, stateful=False, return_sequences=retSeq,\n",
    "                         dropout=dropoutRate, kernel_regularizer=regularizers.l2(0.05))\n",
    "        if isBidirectional:\n",
    "            out = Bidirectional(lstmLayer, merge_mode='concat')(out)\n",
    "        else:\n",
    "            out = lstmLayer(out)\n",
    "\n",
    "    hidden3 = Dense(num_classes, activation='linear')(out)\n",
    "    networkOutput = Softmax()(hidden3)\n",
    "\n",
    "    model1 = Model(inputs=networkInput, outputs=networkOutput)\n",
    "    model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model1\n",
    "\n",
    "lstmModel = createLstmModel(x_train_2.shape[1:], 2)\n",
    "lstmModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Training\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 500\n",
    "\n",
    "\n",
    "history = lstmModel.fit(x_train_2, y_train_2,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test_2, y_test_2),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.set_title(\"Simple LSTM with encoded feat, arithmetic\")\n",
    "ax.set_ylim((0.4,1))\n",
    "ax.set_yticks(np.arange(0.4,1,0.05))\n",
    "ax.grid()\n",
    "ax.plot(history.history['accuracy'])\n",
    "ax.plot(history.history['val_accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
